{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-01-17T16:11:03.922038Z","iopub.execute_input":"2023-01-17T16:11:03.922452Z","iopub.status.idle":"2023-01-17T16:11:03.930456Z","shell.execute_reply.started":"2023-01-17T16:11:03.922419Z","shell.execute_reply":"2023-01-17T16:11:03.929094Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"/kaggle/input/pima-indians-diabetes-database/diabetes.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## credits to  Alankritamishra\n##  https://www.kaggle.com/code/alankritamishra/pytorchann\n## https://github.com/Bjarten/early-stopping-pytorch.git\n## Patrick Loeber","metadata":{"execution":{"iopub.status.busy":"2023-01-17T16:11:05.329655Z","iopub.execute_input":"2023-01-17T16:11:05.330451Z","iopub.status.idle":"2023-01-17T16:11:05.335268Z","shell.execute_reply.started":"2023-01-17T16:11:05.330413Z","shell.execute_reply":"2023-01-17T16:11:05.333792Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"## Note you can play with optimizer and hidden layer for better results.","metadata":{"execution":{"iopub.status.busy":"2023-01-17T16:11:06.176815Z","iopub.execute_input":"2023-01-17T16:11:06.177676Z","iopub.status.idle":"2023-01-17T16:11:06.181907Z","shell.execute_reply.started":"2023-01-17T16:11:06.177636Z","shell.execute_reply":"2023-01-17T16:11:06.180910Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2023-01-17T16:11:06.707114Z","iopub.execute_input":"2023-01-17T16:11:06.708266Z","iopub.status.idle":"2023-01-17T16:11:06.713281Z","shell.execute_reply.started":"2023-01-17T16:11:06.708213Z","shell.execute_reply":"2023-01-17T16:11:06.712203Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/pima-indians-diabetes-database/diabetes.csv')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-01-17T16:11:07.254483Z","iopub.execute_input":"2023-01-17T16:11:07.255187Z","iopub.status.idle":"2023-01-17T16:11:07.293971Z","shell.execute_reply.started":"2023-01-17T16:11:07.255136Z","shell.execute_reply":"2023-01-17T16:11:07.292637Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n0            6      148             72             35        0  33.6   \n1            1       85             66             29        0  26.6   \n2            8      183             64              0        0  23.3   \n3            1       89             66             23       94  28.1   \n4            0      137             40             35      168  43.1   \n\n   DiabetesPedigreeFunction  Age  Outcome  \n0                     0.627   50        1  \n1                     0.351   31        0  \n2                     0.672   32        1  \n3                     0.167   21        0  \n4                     2.288   33        1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Pregnancies</th>\n      <th>Glucose</th>\n      <th>BloodPressure</th>\n      <th>SkinThickness</th>\n      <th>Insulin</th>\n      <th>BMI</th>\n      <th>DiabetesPedigreeFunction</th>\n      <th>Age</th>\n      <th>Outcome</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>6</td>\n      <td>148</td>\n      <td>72</td>\n      <td>35</td>\n      <td>0</td>\n      <td>33.6</td>\n      <td>0.627</td>\n      <td>50</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>85</td>\n      <td>66</td>\n      <td>29</td>\n      <td>0</td>\n      <td>26.6</td>\n      <td>0.351</td>\n      <td>31</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8</td>\n      <td>183</td>\n      <td>64</td>\n      <td>0</td>\n      <td>0</td>\n      <td>23.3</td>\n      <td>0.672</td>\n      <td>32</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>89</td>\n      <td>66</td>\n      <td>23</td>\n      <td>94</td>\n      <td>28.1</td>\n      <td>0.167</td>\n      <td>21</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>137</td>\n      <td>40</td>\n      <td>35</td>\n      <td>168</td>\n      <td>43.1</td>\n      <td>2.288</td>\n      <td>33</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2023-01-17T16:11:07.871258Z","iopub.execute_input":"2023-01-17T16:11:07.871679Z","iopub.status.idle":"2023-01-17T16:11:07.900942Z","shell.execute_reply.started":"2023-01-17T16:11:07.871645Z","shell.execute_reply":"2023-01-17T16:11:07.899799Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 768 entries, 0 to 767\nData columns (total 9 columns):\n #   Column                    Non-Null Count  Dtype  \n---  ------                    --------------  -----  \n 0   Pregnancies               768 non-null    int64  \n 1   Glucose                   768 non-null    int64  \n 2   BloodPressure             768 non-null    int64  \n 3   SkinThickness             768 non-null    int64  \n 4   Insulin                   768 non-null    int64  \n 5   BMI                       768 non-null    float64\n 6   DiabetesPedigreeFunction  768 non-null    float64\n 7   Age                       768 non-null    int64  \n 8   Outcome                   768 non-null    int64  \ndtypes: float64(2), int64(7)\nmemory usage: 54.1 KB\n","output_type":"stream"}]},{"cell_type":"code","source":"sns.countplot(df['Outcome'])","metadata":{"execution":{"iopub.status.busy":"2023-01-17T16:11:08.448150Z","iopub.execute_input":"2023-01-17T16:11:08.448865Z","iopub.status.idle":"2023-01-17T16:11:08.616036Z","shell.execute_reply.started":"2023-01-17T16:11:08.448831Z","shell.execute_reply":"2023-01-17T16:11:08.614325Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n  FutureWarning\n","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"<AxesSubplot:xlabel='Outcome', ylabel='count'>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAAYUAAAEGCAYAAACKB4k+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAAAPPklEQVR4nO3de6xlZXnH8e8PRsQbcpnTKc4MHVPHGoyKdEKx9g8LrQXaOtSA0aiMOMk0KTVam7bUNLU1NdFWpaAN6aQgA6EqXhmNaUsGL60F9aDItZaRiswEmJGbWost+PSP/Z7XDRxgj8w6+zDn+0l29rue9a51njM5mV/WZa+dqkKSJID9pt2AJGnxMBQkSZ2hIEnqDAVJUmcoSJK6ZdNu4PFYvnx5rVmzZtptSNITylVXXfXdqpqZb90TOhTWrFnD7OzstNuQpCeUJLc80jpPH0mSOkNBktQZCpKkzlCQJHWGgiSpMxQkSd2goZDk20muTXJ1ktlWOzTJZUluau+HtHqSnJNke5Jrkhw9ZG+SpIdbiCOFX62qo6pqXVs+E9hWVWuBbW0Z4ERgbXttAs5dgN4kSWOmcfpoPbCljbcAJ4/VL6yRK4GDkxw+hf4kacka+hPNBfxLkgL+vqo2Ayuq6ra2/nZgRRuvBG4d23ZHq902ViPJJkZHEhxxxBGPu8Ff/KMLH/c+tO+56m9Om3YL0lQMHQq/UlU7k/wMcFmS/xhfWVXVAmNiLVg2A6xbt86vjZOkvWjQ00dVtbO97wI+CRwD3DF3Wqi972rTdwKrxzZf1WqSpAUyWCgkeVqSZ8yNgZcD1wFbgQ1t2gbg0jbeCpzW7kI6Frh37DSTJGkBDHn6aAXwySRzP+cfq+qfknwVuCTJRuAW4FVt/meBk4DtwA+B0wfsTZI0j8FCoapuBl40T/1O4Ph56gWcMVQ/kqTH5ieaJUmdoSBJ6gwFSVJnKEiSOkNBktQZCpKkzlCQJHWGgiSpMxQkSZ2hIEnqDAVJUmcoSJI6Q0GS1BkKkqTOUJAkdYaCJKkzFCRJnaEgSeoMBUlSZyhIkjpDQZLUGQqSpM5QkCR1hoIkqTMUJEmdoSBJ6gwFSVJnKEiSOkNBktQZCpKkzlCQJHWGgiSpGzwUkuyf5OtJPtOWn53ky0m2J/lIkgNa/clteXtbv2bo3iRJD7YQRwpvBm4cW343cFZVPQe4G9jY6huBu1v9rDZPkrSABg2FJKuA3wT+oS0HOA74WJuyBTi5jde3Zdr649t8SdICGfpI4W+BPwZ+3JYPA+6pqvvb8g5gZRuvBG4FaOvvbfMfJMmmJLNJZnfv3j1g65K09AwWCkl+C9hVVVftzf1W1eaqWldV62ZmZvbmriVpyVs24L5fCrwiyUnAgcBBwNnAwUmWtaOBVcDONn8nsBrYkWQZ8EzgzgH7kyQ9xGBHClX1p1W1qqrWAK8GLq+q1wKfA05p0zYAl7bx1rZMW395VdVQ/UmSHm4an1P4E+CtSbYzumZwXqufBxzW6m8FzpxCb5K0pA15+qirqs8Dn2/jm4Fj5plzH3DqQvQjSZqfn2iWJHWGgiSpMxQkSZ2hIEnqDAVJUmcoSJI6Q0GS1BkKkqTOUJAkdYaCJKkzFCRJnaEgSeoMBUlSZyhIkjpDQZLUGQqSpM5QkCR1hoIkqTMUJEmdoSBJ6gwFSVJnKEiSOkNBktQZCpKkzlCQJHWGgiSpMxQkSZ2hIEnqDAVJUmcoSJI6Q0GS1BkKkqRusFBIcmCSryT5RpLrk/xlqz87yZeTbE/ykSQHtPqT2/L2tn7NUL1JkuY35JHCj4DjqupFwFHACUmOBd4NnFVVzwHuBja2+RuBu1v9rDZPkrSABguFGvlBW3xSexVwHPCxVt8CnNzG69sybf3xSTJUf5Kkhxv0mkKS/ZNcDewCLgO+BdxTVfe3KTuAlW28ErgVoK2/FzhsyP4kSQ82aChU1QNVdRSwCjgGeN7j3WeSTUlmk8zu3r378e5OkjRmQe4+qqp7gM8BLwEOTrKsrVoF7GzjncBqgLb+mcCd8+xrc1Wtq6p1MzMzQ7cuSUvKkHcfzSQ5uI2fAvw6cCOjcDilTdsAXNrGW9sybf3lVVVD9SdJerhljz3lp3Y4sCXJ/ozC55Kq+kySG4APJ/kr4OvAeW3+ecBFSbYDdwGvHrA3SdI8JgqFJNuq6vjHqo2rqmuAF89Tv5nR9YWH1u8DTp2kH0nSMB41FJIcCDwVWJ7kEGDuFtGD+MldQ5KkfcRjHSn8LvAW4FnAVfwkFL4HfGC4tiRJ0/CooVBVZwNnJ3lTVb1/gXqSJE3JRNcUqur9SX4ZWDO+TVVdOFBfkqQpmPRC80XAzwNXAw+0cgGGgiTtQya9JXUdcKSfG5CkfdukH167DvjZIRuRJE3fpEcKy4EbknyF0SOxAaiqVwzSlSRpKiYNhb8YsglJD/edd7xg2i1oETriz68ddP+T3n30hUG7kCQtCpPeffR9RncbARzA6Atz/ruqDhqqMUnSwpv0SOEZc+P2bWjrgWOHakqSNB17/Ojs9jWbnwJ+Y++3I0mapklPH71ybHE/Rp9buG+QjiRJUzPp3Ue/PTa+H/g2o1NIkqR9yKTXFE4fuhFJ0vRNdE0hyaokn0yyq70+nmTV0M1JkhbWpBeaP8joO5Sf1V6fbjVJ0j5k0lCYqaoPVtX97XUBMDNgX5KkKZg0FO5M8rok+7fX64A7h2xMkrTwJg2FNwKvAm4HbgNOAd4wUE+SpCmZ9JbUdwAbqupugCSHAu9hFBaSpH3EpEcKL5wLBICqugt48TAtSZKmZdJQ2C/JIXML7Uhh0qMMSdITxKT/sb8XuCLJR9vyqcA7h2lJkjQtk36i+cIks8BxrfTKqrphuLYkSdMw8SmgFgIGgSTtw/b40dmSpH2XoSBJ6gwFSVJnKEiSOkNBktQZCpKkbrBQSLI6yeeS3JDk+iRvbvVDk1yW5Kb2fkirJ8k5SbYnuSbJ0UP1Jkma35BHCvcDf1hVRwLHAmckORI4E9hWVWuBbW0Z4ERgbXttAs4dsDdJ0jwGC4Wquq2qvtbG3wduBFYC64EtbdoW4OQ2Xg9cWCNXAgcnOXyo/iRJD7cg1xSSrGH0VNUvAyuq6ra26nZgRRuvBG4d22xHqz10X5uSzCaZ3b1793BNS9ISNHgoJHk68HHgLVX1vfF1VVVA7cn+qmpzVa2rqnUzM34jqCTtTYOGQpInMQqEi6vqE618x9xpofa+q9V3AqvHNl/VapKkBTLk3UcBzgNurKr3ja3aCmxo4w3ApWP109pdSMcC946dZpIkLYAhvyjnpcDrgWuTXN1qbwPeBVySZCNwC6Pvfgb4LHASsB34IXD6gL1JkuYxWChU1b8BeYTVx88zv4AzhupHkvTY/ESzJKkzFCRJnaEgSeoMBUlSZyhIkjpDQZLUGQqSpM5QkCR1hoIkqTMUJEmdoSBJ6gwFSVJnKEiSOkNBktQZCpKkzlCQJHWGgiSpMxQkSZ2hIEnqDAVJUmcoSJI6Q0GS1BkKkqTOUJAkdYaCJKkzFCRJnaEgSeoMBUlSZyhIkjpDQZLUGQqSpM5QkCR1g4VCkvOT7Epy3Vjt0CSXJbmpvR/S6klyTpLtSa5JcvRQfUmSHtmQRwoXACc8pHYmsK2q1gLb2jLAicDa9toEnDtgX5KkRzBYKFTVF4G7HlJeD2xp4y3AyWP1C2vkSuDgJIcP1ZskaX4LfU1hRVXd1sa3AyvaeCVw69i8Ha32MEk2JZlNMrt79+7hOpWkJWhqF5qrqoD6KbbbXFXrqmrdzMzMAJ1J0tK10KFwx9xpofa+q9V3AqvH5q1qNUnSAlroUNgKbGjjDcClY/XT2l1IxwL3jp1mkiQtkGVD7TjJh4CXAcuT7ADeDrwLuCTJRuAW4FVt+meBk4DtwA+B04fqS5L0yAYLhap6zSOsOn6euQWcMVQvkqTJ+IlmSVJnKEiSOkNBktQZCpKkzlCQJHWGgiSpMxQkSZ2hIEnqDAVJUmcoSJI6Q0GS1BkKkqTOUJAkdYaCJKkzFCRJnaEgSeoMBUlSZyhIkjpDQZLUGQqSpM5QkCR1hoIkqTMUJEmdoSBJ6gwFSVJnKEiSOkNBktQZCpKkzlCQJHWGgiSpMxQkSZ2hIEnqDAVJUreoQiHJCUm+mWR7kjOn3Y8kLTWLJhSS7A/8HXAicCTwmiRHTrcrSVpaFk0oAMcA26vq5qr6X+DDwPop9yRJS8qyaTcwZiVw69jyDuCXHjopySZgU1v8QZJvLkBvS8Vy4LvTbmIxyHs2TLsFPZh/m3Penr2xl597pBWLKRQmUlWbgc3T7mNflGS2qtZNuw/pofzbXDiL6fTRTmD12PKqVpMkLZDFFApfBdYmeXaSA4BXA1un3JMkLSmL5vRRVd2f5PeBfwb2B86vquun3NZS42k5LVb+bS6QVNW0e5AkLRKL6fSRJGnKDAVJUmcoyMeLaNFKcn6SXUmum3YvS4WhsMT5eBEtchcAJ0y7iaXEUJCPF9GiVVVfBO6adh9LiaGg+R4vsnJKvUiaMkNBktQZCvLxIpI6Q0E+XkRSZygscVV1PzD3eJEbgUt8vIgWiyQfAq4AfiHJjiQbp93Tvs7HXEiSOo8UJEmdoSBJ6gwFSVJnKEiSOkNBktQZClrykqxKcmmSm5J8K8nZ7TMbj7bN2xaqP2khGQpa0pIE+ATwqapaCzwXeDrwzsfY1FDQPslQ0FJ3HHBfVX0QoKoeAP4AeGOS30vygbmJST6T5GVJ3gU8JcnVSS5u605Lck2SbyS5qNXWJLm81bclOaLVL0hybpIrk9zc9nl+khuTXDD2816e5IokX0vy0SRPX7B/FS1ZhoKWuucDV40Xqup7wHeAZfNtUFVnAv9TVUdV1WuTPB/4M+C4qnoR8OY29f3Alqp6IXAxcM7Ybg4BXsIogLYCZ7VeXpDkqCTL2z5/raqOBmaBt+6NX1h6NPP+0UvaI8cBH62q7wJU1dzz/18CvLKNLwL+emybT1dVJbkWuKOqrgVIcj2whtGDCY8EvjQ6w8UBjB73IA3KUNBSdwNwynghyUHAEcA9PPho+sC9+HN/1N5/PDaeW14GPABcVlWv2Ys/U3pMnj7SUrcNeGqS06B/Pel7GX0N5M3AUUn2S7Ka0bfUzfm/JE9q48uBU5Mc1vZxaKv/O6OnzgK8FvjXPejrSuClSZ7T9vm0JM/d019O2lOGgpa0Gj0R8ncY/ad+E/CfwH2M7i76EvBfjI4mzgG+NrbpZuCaJBe3p8q+E/hCkm8A72tz3gScnuQa4PX85FrDJH3tBt4AfKhtfwXwvJ/295Qm5VNSJUmdRwqSpM5QkCR1hoIkqTMUJEmdoSBJ6gwFSVJnKEiSuv8HHGGod05G2B8AAAAASUVORK5CYII=\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":"sns.FacetGrid(df , hue = 'Outcome' , size = 5 ).map(sns.distplot , 'Age').add_legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-01-17T16:11:08.929096Z","iopub.execute_input":"2023-01-17T16:11:08.929825Z","iopub.status.idle":"2023-01-17T16:11:09.409112Z","shell.execute_reply.started":"2023-01-17T16:11:08.929785Z","shell.execute_reply":"2023-01-17T16:11:09.408166Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/seaborn/axisgrid.py:337: UserWarning: The `size` parameter has been renamed to `height`; please update your code.\n  warnings.warn(msg, UserWarning)\n/opt/conda/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n  warnings.warn(msg, FutureWarning)\n/opt/conda/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n  warnings.warn(msg, FutureWarning)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 413.25x360 with 1 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAAZMAAAFgCAYAAAB+GpTWAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAAA5JklEQVR4nO3deXxcdb3/8ddnsu979zZJd8rSQssOIiDIjggo4IKK4r3XBeXqvfD7eb3oVX/uXK/iFRUREGQXKiDIJihLaSllSRdS2rRNumRr9j3z/f1xJhBC2kxz5sxkeT8fxsycc2bm02SYd873+z3frznnEBER8SOU6AJERGT8U5iIiIhvChMREfFNYSIiIr4pTERExLfkRBcQK2eccYZ79NFHE12GiMhglugC4mXCnJnU19cnugQRkUlrwoSJiIgkjsJERER8U5iIiIhvChMREfFNYSIiIr4pTERExDeFiYiI+KYwERER3xQmIiLim8JERER8U5iIiIhvChMREfFNYSIiIr5NmCnoJ4I7Vm2P+tjLjp4TYCUiIgdGZyYiIuKbwkRERHxTmIiIiG8KExER8U1hIiIivilMRETEN4WJiIj4pjARERHfFCYiIuKbwkRERHxTmIiIiG8KExER8U1hIiIivilMRETEN4WJiIj4pjARERHfFCYiIuKbwkRERHxTmIiIiG8KExER8U1hIiIivilMRETEN4WJiIj4pjARERHfFCYiIuKbwkRERHxTmIiIiG8KExER8U1hIiIivgUaJmZ2hpltMrPNZnbNMPvfZ2ZrzazPzC4asu9yM6uMfF0eZJ0iIuJPYGFiZknADcCZwBLgUjNbMuSw7cCngDuGPLYQ+E/gaOAo4D/NrCCoWkVExJ8gz0yOAjY757Y453qAO4HzBx/gnKtyzr0GhIc89oPA4865RufcXuBx4IwAaxURER+CDJOZwI5B96sj22L2WDO70szWmNmaurq6URcqIiL+jOsOeOfcr51zK5xzK0pKShJdjojIpBVkmNQAswfdnxXZFvRjRUQkzoIMk9XAAjMrN7NU4BJgZZSPfQw43cwKIh3vp0e2iYjIGBRYmDjn+oAv4oXABuBu51yFmX3bzM4DMLMjzawauBi40cwqIo9tBP4LL5BWA9+ObBMRkTEoOcgnd849AjwyZNs3B91ejdeENdxjfwf8Lsj6REQkNsZ1B7yIiIwNChMREfFNYSIiIr4pTERExDeFiYiI+KYwERER3xQmIiLim8JERER8U5iIiIhvChMREfFNYSIiIr4pTERExDeFiYiI+KYwERER3xQmIiLim8JERER8U5iIiIhvChMREfFNYSIiIr4pTERExDeFiYiI+KYwERER3xQmIiLim8JERER8U5iIiIhvChMREfFNYSIiIr4pTERExDeFiYiI+KYwERER3xQmIiLim8JERER8U5iIiIhvChMREfFNYSIiIr4pTERExDeFiYiI+KYwERER3xQmIiLim8JERER8U5iIiIhvChMREfFNYSIiIr4FGiZmdoaZbTKzzWZ2zTD708zsrsj+VWZWFtmeYma3mNnrZrbBzK4Nsk4REfEnsDAxsyTgBuBMYAlwqZktGXLYFcBe59x84HrgB5HtFwNpzrlDgeXA5weCRkRExp4gz0yOAjY757Y453qAO4HzhxxzPnBL5Pa9wKlmZoADsswsGcgAeoCWAGsVEREfggyTmcCOQferI9uGPcY51wc0A0V4wdIO7AK2Az92zjUOfQEzu9LM1pjZmrq6utj/C0REJCpjtQP+KKAfmAGUA/9qZnOHHuSc+7VzboVzbkVJSUm8axQRkYggw6QGmD3o/qzItmGPiTRp5QENwGXAo865XudcLfAcsCLAWkVExIcgw2Q1sMDMys0sFbgEWDnkmJXA5ZHbFwFPOeccXtPWKQBmlgUcA2wMsFYREfEhsDCJ9IF8EXgM2ADc7ZyrMLNvm9l5kcNuAorMbDNwNTAwfPgGINvMKvBC6Wbn3GtB1SoiIv4kB/nkzrlHgEeGbPvmoNtdeMOAhz6ubbjtIiIyNo3VDngRERlHFCYiIuKbwkRERHxTmIiIiG8KExER8U1hIiIivilMRETEN4WJiIj4pjARERHfFCYiIuKbwkRERHxTmIiIiG8KExER8U1hIiIivilMRETEN4WJiIj4pjARERHfFCYiIuKbwkRERHxTmIiIiG8KExER8U1hIiIivilMRETEN4WJiIj4pjARERHfFCYiIuKbwkRERHxTmIiIiG8KExER8U1hIiIivilMRETEN4WJiIj4FlWYmNn9Zna2mSl8RETkPaINh18ClwGVZvZ9M1sUYE0iIjLORBUmzrknnHMfA44AqoAnzOx5M/u0maUEWaCIiIx9UTdbmVkR8Cngs8ArwM/wwuXxQCoTEZFxIzmag8zsT8Ai4DbgXOfcrsiuu8xsTVDFiYjI+BBVmAC/cc49MniDmaU557qdcysCqEtERMaRaJu5vjPMthdiWYiIiIxf+z0zMbNpwEwgw8wOByyyKxfIDLg2EREZJ0Zq5vogXqf7LOCng7a3Av8noJpERGSc2W+YOOduAW4xswudc/fFqSYRERlnRmrm+rhz7g9AmZldPXS/c+6nwzxMREQmmZE64LMi37OBnGG+9svMzjCzTWa22cyuGWZ/mpndFdm/yszKBu07zMxeMLMKM3vdzNKj/UeJiEh8jdTMdWPk+7cO9InNLAm4ATgNqAZWm9lK59z6QYddAex1zs03s0uAHwAfNbNk4A/AJ5xzr0YumOw90BpERMYqM5uF9xm5BO8P+4eArzvnevbzmP/jnPtenEo8INFO9PhDM8s1sxQze9LM6szs4yM87Chgs3NuS+SHcydw/pBjzgduidy+FzjVzAw4HXjNOfcqgHOuwTnXH+0/SkRkLIt8zt0PPOCcWwAsxGsB+u4IDx2zA5+ivc7kdOdcC3AO3txc84Gvj/CYmcCOQferI9uGPcY51wc0A0V4P1hnZo+Z2Voz+7fhXsDMrjSzNWa2pq6uLsp/iohIwp0CdDnnbgaI/LH8VeAzZvYvZvaLgQPN7CEze7+ZfR/vMo11ZnZ7ZN8nzew1M3vVzG6LbCszs6ci2580szmR7b83s/81sxfNbEvkOX9nZhvM7PeDXu/0SBfDWjO7x8yyo/kHRRsmA81hZwP3OOeao3zcaCUDJwAfi3y/wMxOHXqQc+7XzrkVzrkVJSUlAZeUWGHneHFLA794upK71+xg/c6WRJckIqN3MPDy4A2RP9i3s4/uB+fcNUCnc26Zc+5jZnYw8A3gFOfcUuCqyKE/B25xzh0G3A78z6CnKQCOxQuulcD1kVoONbNlZlYcec4POOeOANYA7xl8NZxow+QhM9sILAeeNLMSoGuEx9QAswfdnxXZNuwxkX6SPKAB7yzmWedcvXOuA3gEb1LJSev+tdWsfHUnzsGm3a1c+psX2bhbgSIyiZ2C98d9PYBzrjGy/Vjgjsjt2/D+IB/wZ+ecA14H9jjnXnfOhYEKoAw4Bq8P5zkzWwdcDpRGU0y0U9BfAxwHrHDO9QLtvLf/Y6jVwAIzKzezVOASvCQcbGWkWICLgKci/9DH8JIyMxIyJwHrmaTW72xh7fYmTlpYwhdPns8XT55PRkoSn7l5NZ096koSGYfW4/1x/jYzywXmAE28+7M5liNZuyPfw4NuD9xPxpvl5PHI2c8y59wS59wV0TzxgaycuBhvpNUn8T74T9/fwZE+kC/iBcMG4G7nXIWZfdvMzoscdhNQZGab8U6lrok8di/eFfergXXAWufcwwdQ64TRFw6z8tUapuelc+pBUzAzCrJS+fllh7OzuYvf/H1LoksUkQP3JJAZ+TwdGP36E+D3wBZgmZmFzGw23mCmAb2D1pB6Crg4MtoVMyuMbH8e74938LoK/n4Adb0IHG9m8yPPmWVmC6N5YLRT0N8GzMP7YB/4U9gBt+7vcZGZhh8Zsu2bg253ARfv47F/wBsePKlV7GyhpauPCw6fRXLonew/sqyQMw+Zxq+eeYtLj5pDSU5aAqsUkQPhnHNmdgHwSzP7D7w/7B/BG63VA2zFO3vZAKwd9NBfA6+Z2dpIv8l3gWfMrB9vnalPAV8CbjazrwN1wKcPoK46M/sU8EczG/hQ+Qbw5kiPNa9VaYSDzDYAS1w0ByfIihUr3Jo143tplTtWbX/PthuffYvWrj6uPm0hIbO3t1929By21LVxyk+e4erTFvLlUxfEs1QRiY6NfMjEEG0z1xvAtCALkffa3dLFtoYOji4vfFeQDJhbks2JC4r540vb6esPJ6BCERFPtGFSDKyPXPexcuAryMIEKmqaMeDwOQX7PObjx5Syq7mLJzbUxq8wEZEhol1p8bogi5DhbdjdwuzCTLLT9v1rOnXxFKbmpnH/2mrOOEQnjyKSGNEODX4G78r3lMjt1by7U0hirLmzl51NXRw0bf/zaSYnhTjr0On87c06Wrs0fZmIJEa0c3N9Dm/urBsjm2YCDwRUk8DbFyQunp474rHnHDadnr4wT6qpS0QSJNo+ky8AxwMtAM65SmBKUEUJVO5poyAzhSlRDPk9fHYB0/PSeei1nXGoTETkvaINk+7B0yJHrkofs8OEx7uwc1Q1tFNenI0NM4prqFDIOOOQaTxbWa8r4kXkgIy07lS0ou2Af8bM/g/ejJWnAf8C/Hm0Lyr7V9faTUdPP+XFmVE/5tTFU7n5uSqef6ueUw+aGmB1IhKEsmsevjKWz1f1/bN/PdIxUa47FZVoz0yuwbuS8nXg83hXan7jQF9MolPV0A5AWVHWCEe+48jyAjJTk3h6k/pNRCRq0aw7FZWozkycc2EzewBvIRctHBKwrfXt5KQnU5iVGvVj0pKTOGF+MU9vrMM5F1XzmIhMesOtO3X0aJ5ov2cm5rnOzOqBTcCmyCqL39zf48SfbQ0dlBVlHXAgnLx4CjVNnVTWtgVUmYjI8EZq5voq3iiuI51zhc65QrzUOt7Mvhp4dZNQS1cvzZ29zCmMvr9kwIkLigF4fnN9rMsSkYkpmnWnojJSmHwCuNQ5t3Vgg3NuC/Bx4JOjeUHZv517OwGYmZ9xwI+dVZDJ7MIMXtjSEOuyRGRiimbdqaiMFCYpA6t4DRbpN0kZ5njxqbqpEwOm549uPZxj5xaxamsj4bBGbovI/u1r3anRPNdIHfA9o9wno1Szt5OSnDTSkpNG9fhj5xVx95pqNuxu4eAZeTGuTkSCEs1Q3iAMt+7UaIwUJkvNbLiFxo3YLiUpgHOOmqZOFk7NHvVzHDvX6zd54a0GhYmIxM1+m7mcc0nOudxhvnKcc2rmirGWrj7auvtG1V8yYFpeOuXFWbyofhMRiaMDWQNeArazafSd74MdE+k36Ve/iYjEicJkDNnV3AXA1Dx/LYjHziuitauPip3NsShLRGRECpMxZE9LF4VZqaPufB9wzNxCwOs3ERGJh2gnepQ42N3cxbTc6M5K7li1fb/7S3LSuG9tNTnpKVx29JxYlCcisk86Mxkjunr7qW/rZmqUYTKSucVZbGvoIOzUbyIiwzOz35lZrZm94fe5dGYyRmyubcPhjcaKhdKiTFZtbWR3pB9GRMa46/JiOgU91zVHc93K74FfALf6fTmdmYwRG3Z5l/NE28w1ktJCb/r67Y0dMXk+EZl4nHPPAo2xeC6FyRixcXcrySGjKDv6aef3Jz8zhdz0ZLZF1kYREQmSwmSMqKxtY0pOGqEYrUNiZswpymKbzkxEJA4UJmPEW7VtlOSkxfQ5SwszaeroVb+JiAROYTIGtHf3UdPUyZQY9ZcMKC3y1kRZsy0mTaIiIvukMBkD3qrzVkYsyY7tmcn0vAxSkow1VXtj+rwiMjGY2R+BF4BFZlZtZleM9rk0NHgM2BxZZndKjJu5kkLGrIJM1m5XmIiMedEN5Y0p59ylsXounZmMAZtr2yIjuWIbJuA1dVXsbKGjpy/mzy0iMkBhMgZU1rZRVpxFUig2I7kGKy3MpD/sWLejKebPLSIyQGEyBrxV28b8ktEviLU/cyIXL76sfhMRCZDCJMF6+8Nsa+xg3pSsQJ4/IzWJhVOzWbNNYSIiwVGYJFj13k76w46yomDCBGB5aSFrt+8lrMWyRCQgCpMEq6r3pjspLw4uTFaUFtDa1UdlZNSYiEisKUwSbGskTMoCDJPlpQWALl4UkeAoTBKsqqGdnLRkirJiM8HjcEqLMinOTlUnvIgERmGSYFvr2ykrzsJiNMHjcMyM5aUF6oQXkcAoTBJsIEyCtqK0kO2NHdS2atJHEYk9hUkCdff1s7Opk/LIhIxBWl7m9ZuoqUtEgqAwSaAdjR2EXbCd7wMOmZFHWnJITV0iEohAw8TMzjCzTWa22cyuGWZ/mpndFdm/yszKhuyfY2ZtZva1IOtMlK313sJV8QiT1OQQS2fnK0xEJBCBhYmZJQE3AGcCS4BLzWzJkMOuAPY65+YD1wM/GLL/p8Bfgqox0d6+xiTACxYHW1FaQEVNM509/XF5PRGZPII8MzkK2Oyc2+Kc6wHuBM4fcsz5wC2R2/cCp1pkWJOZfQjYClQEWGNCbW1oJz8zhYIAhwUPtqKsgD5N+igiAQgyTGYCOwbdr45sG/YY51wf0AwUmVk28O/At/b3AmZ2pZmtMbM1dXV1MSs8Xqrq2wOdRmWoI+ZEOuF18aKIxNhY7YC/DrjeObff+T+cc792zq1wzq0oKSmJT2UxVFXfHug0KkPlZ6ayYIomfRSR2AtypcUaYPag+7Mi24Y7ptrMkoE8oAE4GrjIzH4I5ANhM+tyzv0iwHrjqqu3n53NXXE9MwFYUVbIw6/tJBx2hAJYP0VEJqcgz0xWAwvMrNzMUoFLgJVDjlkJXB65fRHwlPOc6Jwrc86VAf8NfG8iBQnAtoaBkVzBX2My2IrSAlo06aOIxFhgYRLpA/ki8BiwAbjbOVdhZt82s/Mih92E10eyGbgaeM/w4YlqaxxmCx7OisjFi6ur1G8iIrETZDMXzrlHgEeGbPvmoNtdwMUjPMd1gRSXYFUNwc8WPJw5hZkUZ6fx8ra9fPyY0ri+tohMXGO1A37Cq6pvpygrldz0lLi+rpmxorRA09GLSEwpTBIkXhM8DmdFWQE7GjupbdGkjyISGwqTBKlqiO81JoOtKCsE0BBhEYkZhUkCdPT0saelm/I4j+QacPCMXNJTQuqEF5GYUZgkQFUcJ3gcTkpSiKWz8nlZZyYiEiMKkwR4eyRXgpq5AI4uL6RiZwutXb0Jq0FEJg6FSQIMXGOSqDMTgGPmFtEfdqzRYlkiEgOBXmciw6uqb6ckJ43stOB//DntVfD07bDtOdhbBeE+yCrhqKmHcU5yCas3z+TkxVMCr0NEJjaFSQJUNbQHvoZJelcdR2z8EaW7HvU2zDgcSo+HpGRo2UXypof4RXITe1++GYqvheWfgpT0QGsSkYlLYZIAW+s7OGXxe2c5nrf9npg8f17bW8yvvo9QuJedxccxc9kHIT33nQNmAovP5r4125i+83GOe/Tf4W//Dw77CJz1o5jUICKTi8Ikzlq7eqlv6w6sv6S46VXm1qykM62EytkX05VWxMzBQTLAQkwvXchl247m/sWrOGL3XfDSr71msNO/A6mJ688RkfFHYRJnA8OC5wYQJoXNFcyteZCWrHLenP1RwkneCo6rtg5/PUl/2EixPH5XO5++WZ9hdu1TTF9zM2xfBZfeAQVlMa9xRGtu9vf4FZ+OTR0ickA0mivOtgY0wWN2xw7m1TxAa+YcNs255O0g2Z/UkGNBVhcVrVm4UDLbp50OH78PWqrh1yfD1mdjWqOITFwKkzirigwLLi2MXZgk93Uwf8d99CTnUjn7I7hQ9JNHLslpp6ojjfa+yFth/qnwuachqwRuuwDW3hqzOkVk4lKYxFlVfTvT89LJSE2KzRM6R/nOh0jpb6Ny9oX0JR/YFC0H53TgMDa2DXpc0Tz47ONQ/j5Y+SV44joIh2NTr4hMSAqTONsa4wkeC1s2UNi6keqSk+nImHHAj5+f1UWKhXm9dUgIpefBZXfD8k/DP66Hez8NvZ0xqlpEJhqFSZxVxXDq+VC4h9Ldj9KePp1dxceO6jlSQ44lOR282jJMTUkpcM713uiu9Q/CLedCW53PqkVkItJorjhq7uhlb0dvzGYLnlb/Iql9bVTOvhhs9H8XLM1t59bqqdR2pww/mio12ztDeeU2uOEoOOpKyJk2/JNpNJXIpKQzkzjaGsMJHpP72pjR8DyNOYtpy5zt67mW5Xl1DXt2MmD6YXDclyDcC8/9N9S/6es1RWRiUZjE0cBIrrkl/sNkVu2zhMK97Jh6qu/nmpHWQ0lqD+uaR6grfw4c/1XIKIBVv4LtL/h+bRGZGBQmcbSlvp2QwexCf81caT2NTNn7MrUFy+lKK/Jdlxksy23njdZMekYatJVZCMd9GYoXwGt3wdpboKfddw0iMr4pTOJoa307MwsySEv2Nyx4ev2LOAtRU3JijCqDpXntdIWTeLk+imtUUjLgyCth0dmw6zV45gdQ/RI4DR8WmawUJnFUVd9OeXG2r+dI7uuguGkd9XmH0ZuSE6PK4JCcDpLM8cyeka+cByCUBAtOgxOv9oYRr7sDnv0hvH4v9HbFrC4RGR8UJnHinGNrfbvvObmmNq4myfWxu+iYGFXmyUgKszi7g7/tjjJMBuTOhBO+CkdcDuF+uO8K+PFCePALXsDUvQl93TGtVUTGHg0NjpO6tm7auvso9xEmFu5lauNq9mYvoDP9vVPY+7U0t507arLY3RliWsYBNFlZyFsvZfpSKJwLr94J61fCK39455i0PEjLBucGPc4iQ5rN25c3G3o7oGgBFC/U+ioi44jCJE621nmd1H7CpLjpNVL6O0Z9geJIlue1cUfNFB7fmcYn5o3iancLwbyTva9w2Bs+vHMttNR4Fzv2tINB5P8A54WLC0NXMzTvgLpNUPV377mmHAzzToHC8hj+K0UkCAqTOBlY933UYeIcU/e+THv6NFozS2NY2TtmZfQwP6ePR6pHGSaDhUIwZbH3dSBe+o23vPCeCq9T//nXYdphcMiFXt+MiIxJ6jOJk6317aQmhZiRnzGqx2d27SKraze1BYd7zUMBOWtWN6vqUqjvCu419iuUDEXzYcn5cMo3YdFZULsBnvk+1G1MTE0iMiKFSZxsrW+ntCiTpNDoPqSn7H2FfkumIe/QGFf2bmfN6iKM8djOtEBfJyrJabDgdHjfv0F6Pqy6Ebb87d39LiIyJqiZK0621rePuokr1N9DcfPrNOYdTH9SsJ3Si3L7mZvdx1+q0/nY3FEM8fW7UuJwskvg+K/Autth/QPQ1QIHnRvoGZqIHBidmcRBf9ixraGD8lFOo1LUUkFSuMdr4gqYGZw5q5sX6lJo7B5DH9bJabD8U1B6Amx5Ct78S6IrEpFBFCZxsLOpk57+8KivMSnZ+wqdacW0Zfib0DFaZ83qpt8Zfx0LTV2DWQgO+TDMORYq/wqbH090RSISoTCJgy1vj+Q68Kvf07obyOmspi5/WdyadZbk9VGa1cfD1WMsTMALlEMvhpnLYePDsPOVRFckIihM4mJgtuCyUaxjUtz8Bg6ozzskxlXtmxmcM7ub5/aksqdzDL5FLASHXQoF5d5V9k3bE12RyKQ3Bj8pJp6t9e1kpyVTkn2Af+k7R1Hz67RkldGbkhtMcftwUak3quv+bWP0KvSkZFhxBaTlwOrfep3yIpIwCpM42BIZyWUH2EyV1bWTjJ7GwIcDD6c8p58ji3q4Z1v62B2Jm5YNR37WW5v+lds0a7FIAilM4mBrfduohgUXNb1B2JJozD0ogKpGdnFZF1tak1nTEMW09ImSOwMOvQgaKuHNRxNdjcikpTAJWGdPP9V7Ow98dUUXpqjlDZqyFwR+bcm+nDO7i9yUMLdsHt1V+3Ez+2iYfRRUPg6bn0h0NSKTksIkYG/VteEcLJx6YGuP5LZXkdrXTn1+/Ju4BmQmw0fKuni0Jm1sdsQPdshFkDMN7r8SmmsSXY3IpDPGPyHGv821bQAsmHJgw4ILWzbQH0qhKXt+EGVF7RPzOul3cNtbY/zsJCnVu6ixrxvu/Qz09ya6IpFJRWESsDf3tJIcMsoOpM/EhSls2UBT9kJcKLH9FaXZ/Zw+o5tb38qgtXcMXRE/nOypcO7PYMeL8OS3El2NyKSiMAlYZa3X+Z6SFP2POqdjOyn9HTTkLQmwsuh9YXEHLb0hbt8yxs9OwOuMX3EFPP9zb4EuEYkLhUnANte2sWDqKJq4LIXmBDdxDTissI8Tp3bzmzczx/7ZCcAZ/w9mHAEP/AvUb050NSKTQqBhYmZnmNkmM9tsZtcMsz/NzO6K7F9lZmWR7aeZ2ctm9nrk+ylB1hmUrt5+tjW0M3/KAXS+DzRx5cwnnIAmrlVbG4f9OqNgFw3dIa57yd7eNmYlp8FHboWkFLj7E94KjyISqMDCxMySgBuAM4ElwKVmNrTd5gpgr3NuPnA98IPI9nrgXOfcocDlwG1B1RmkLXXthB0sPIAzk5yOHaT2tdGYOzaauAbMz+ri2IIWHtpTSGPPOFi5IH82XPgbb2Gth76qNVBEAhbkmclRwGbn3BbnXA9wJ3D+kGPOB26J3L4XONXMzDn3inNuZ2R7BZBhZmNw1sH9q6xtBWDBAZyZFLZsIGzJCR/FNZzLZtYRdnDzjqmJLiU68z8A778WXrsLXrgh0dWITGhBhslMYMeg+9WRbcMe45zrA5qBoiHHXAisdc51D30BM7vSzNaY2Zq6urqYFR4rlXvaSApZ9BM8OhcZxTWPcNLYy84pab1cPKOel5pyeGnvgc+AnBDv+zocdB789RveLMMiEogx3QFvZgfjNX19frj9zrlfO+dWOOdWlJSUxLe4KFTWtlJWlElaclJUx2d3VpPa1zrmmrgGO3tqI2UZXdy4bTo7O8b028cTCsEFN8LMI+C+z0LN2kRXJDIhBflpUAMMXs1pVmTbsMeYWTKQBzRE7s8C/gR80jn3VoB1Bqaytm0UTVxJNOUsDLAqf5INrppbQ68zvrQql67+RFcUhdRMuOSPkFkMt18EtRsTXZHIhBNkmKwGFphZuZmlApcAQwf+r8TrYAe4CHjKOefMLB94GLjGOfdcgDUGpruvn20NHdEPC440cTVnz6N/DDZxDTYjvZd/Lt3Fyw2pfG11LuHx0LedMxU++QCEkuHW86FhXP59IjJmBRYmkT6QLwKPARuAu51zFWb2bTM7L3LYTUCRmW0GrgYGhg9/EZgPfNPM1kW+pgRVaxC21rfTH3YsiHZOrp1rSettTtgMwQfq2MJWrjm0jYeq0/na6hz6xsPs70Xz4JMPQn8P3HIu1L2Z6IpEJoxAx3g65x4BHhmy7ZuDbncBFw/zuO8A3wmytqBV7jnAObkqHiBsIfaO4SauoT6/sIPeMPykIpuG7hD/c3QLealj/DRlykFeoPzhw3DzGfCxe73+FBHxZRz0oI5PlXtaCRnRrWPiHKx/kJasufQnjYMpSyLM4EsHdfC9I1p4vjaVs54o5NndqYkua2TTD4PPPAapWd4ZypuPJboikXFPYRKQjbtbKSvOIj0lipFcu9ZB07Zx08Q11GVzu7j7/XtJS3J88h/5XP73PNY2jPELG4vmwWf+CoVz4Y6PwrM/0oWNIj4oTAJSsbOFg2fkRXfw+gchlExjzuJgiwrQ4UV9PPKBRv79kDZe35vCh58u5LJn8nlwe9rYHfGVO907Qzn0InjqO3DP5dDdluiqRMYlhUkAmjt6qWnqZMn03JEPdg4qHoDy99GfPH6auIaTngT/vLiDv5/VwDWHtrGtPYmrXsrjyIeKufblHF6qSxl7I79SM+HDv4HTvwMb/gy//QDUVya6KpFxZ4y3RYxP63e1ALBkRhRhsvt12LsVTvgqbKkPuLL4yEp2/NOiDq5c2MGLdSncW5XOg9vT+OPWDGZl9nPBnC4+VNrFvJwxcspiBsd9CaYeDHd9HP73eFh6Ccw4fHTPt+LTsa1PZBzQmUkAKnY2A0R3ZrL+AbAkWHxOsEUlQMjguCm9/PSoVlaf28D1RzZTntPHDRszOfWxIi58Op/HalLHztnKvFPgxK95y/+uvQXeuB/CfYmuSmRc0JlJANbvamFKTholOSNcfPh2E9eJkDV0SrKJJSvZcUFpNxeUdlPbGeLBHWnc+lYmn38hn7k5fXxlSTvnzurG/C6XsuZmf4/PKPDOUjashK3PQtM2bzngjAKfhYlMbAqTAKzf2RJdE9eeCmh8y/vwmkSmZIT53MJOPj2/k0dq0vjlxky+vCqP31f28h9LW+lpqY36uY4uL4x9gaFkOPjDUDAXXv0j/P3HcPgnoGT8DpAQCZrCJMa6evuprG3j1IOiuGB//QNgoXHbxHUgC2QN96GfHILzZndz9qxu7qtK50cVWVzwdCFnlMClM+tIT0pw+9eMZd6Ir5d/D6tuhEVnwvzT8H/6JDLxqM8kxip2NtMfdiydlb//AweauMpOgOyxN+NxPCUZfKS8i6c/2Min5nfwaF0h/7ahnI1tY2B0W/ZUb3DEzOWw6RFYdzv0qx9FZCiFSYyt2+F1vi+bnb//A2s3QEMlLBm6XtjklZ3iuG5ZG/+5cBvOwXWb5vCnXYWJ76BPSoVlH4NFZ0HNGlj1v1oKWGQIhUmMvVbdxPS8dKbkpu//wIr7vSaug87b/3GT0JKcTn64pIpjClq5c+cUfrh5Fq19CX6rmsGC072+k6YqeO5n0NGQ2JpExhD1mcTYqzuaOGzWCFe+Owev3wPlJ0H2uJoMedQOpH8FICMpzFXlOzkou4Nbq6fwfzeU8W/zq5mV0RNQhVGauRzS82HNTfD8/8DR/+wNJRaZ5HRmEkNNHT1UNXSwdKQmrurVsLcKDvtIPMoat8zgg1Oa+M+F2+kKh/jGxlJeaY5i4sygFc2DY78ILgzP/xyadoz8GJEJTmESQ+t2NAGwbKTO99fuhuT0cTuKK94WZnfxvYOqmJrWyw82z+LhPQWJn5MxdwYc92VIToUXb9BiWzLpKUxiaHVVI0khY9mc/H0f1N/r9ZcsOhPSo7gWRQAoTu3jW4u2sSK/jVurp3LjtmmJX5ArqwSOu8r7Pa76FdSuT3BBIomjMImh1Vv3csiMXDJT99MV9dZTXsftoWriOlDpSY6r59bw4Wn1PN2Qz3cq59DQneBrPjLy4dgve8sCr/4t7HwlsfWIJIjCJEa6+/pZV93EkWUjXJH92t3e1BzzPxCfwiaYkMFHZ9bz5fIaNrenc/6ThWxqjmLNmCClZcMxX4CCMlh7K6z5XWLrEUkAhUmMvF7dTE9fmCP3N71H517Y+JA3VUfyOFiRcAw7vrCV6xZtpycMH366gCd2JvjnmZIBR/+TtyzwQ1/VYlsy6ShMYuSlKm/o64rS/UwI+Nrd0NcFyy+PU1UT2/ysLlaeupd5Of187vk8frUpM7Gf30mpsOIKOOyj3mJbj14L4UR37IjEh8IkRl54q4GFU7Mpyt7HTMHOwcu3wPRlMH1pXGubyKZlhLnrpL2cPaub77+ezb+uzknsyo6hJPjQr+CYf/GulH/gn7xBFyITnMIkBrp6+1m1tZETF+xnjq2al6G2QmclAchIhp8f3cK/HtzG/dszuPSZAmo7E/jWDoXgg9+DU/4DXrsL/ngJdLUkrh6RONAV8DHw0tZGevrCnLigeN8Hrb0FUjLhkIviV9gkMPjK+qPSG7l6bjY3VM3gtMfy+XL5Tg7J7Xh7fyDT1e+LGbzva97w4YevhptOg0vvhMLy+NUgEkc6M4mBv1fWkZoU4ujyfSxw1dUCr98Hh3xY15YE7OiCNr67uIrs5H6+Uzmbe3cWJXaiyOWXw8fvh9bd8JtToOq5BBYjEhyFSQw8+2Y9R5YXkJG6jyGqr9wGve1e56wEbnZGD99bXMUJhS3cs6uE71XOpqk3gcOH554En3sKMovg1vPhpd9opJdMOAoTn7Y1tLNpTysnL9rHhI39vfDCL6H0BJh5RHyLm8TSkxxfKNvF50t3sbEtg3+tmMuD29MS9xleNA8++wTMOxke+Rrcczl0NSeoGJHYU5+JT49V7AbggwfvY+bYigegpRrO/kn8ihLA67Y4pbiZhVmd/O+26Vz1Uh5/qenivw5vpSQ9wFTZ3zr0C8/w5mXb8Geo+gcccTnkz3n3MSs+HVxtIgHRmYlPj76xm4Nn5DK7MPO9O53zpikvXuithSEJMSujh28v2sY1h7bx1K40Tv9rEfdUpSemL8VCMO8UOPZLEO6H5/4b3nzUuy0yjilMfNjT0sXa7U2csa+zkq3PwO7XvOnKQ/pRJ1KSwT8t6uDhDzRSnt3P19fkcsFTBaxrTNDJeWE5vO/rMP1wL0yeux5adiWmFpEY0CecDw+uqwHgzEOnv3enc/DUdyFnundFtIwJC3L7uffkvfzkyBZ2dob40FOFfP75XCqaEhAqqVlwxCdg+aehswn+8WPY9Bfo7Yp/LSI+KUxGyTnHPWuqOWJOPvOnZL/3gI0PQfVL8P5rIWWEJXwlrkIGF5Z28fQHG/nKkjaer0vl7CcKueK5PJ7ZnRr/5q/pS+Gka2DaUqh8DH55DGx+Is5FiPijMBmlV6ubqaxt4+IVs9+7s78PnvgWFC+CZR+Lf3ESlewUx1eWdPCPMxu46qB21jWmcPk/8jn50UJ+WpFFRVNy/EZ/pWXDEZ/0pmEJJcEfLoS7Pg6NW+JUgIg/Gs01Sn9ctZ30lBDnHDZME9crt0JDJVxyByTpRzxW7G8d+uMyGzhyifFSUzZP1OXz8w2Z/M+GLGZm9rO8qJfDC3s5KL+P2Vn9TE0PkzzCn2H9Djr7jM5+6Oq3yO3IV58RMkgJOVJDkBxypIQgM8mRm7OI3Cu/ScqqX8Dfr4dNj8JRn/P6VzLjeAW/yAHSJ90o1LZ08adXavjIkbPISU95986ORq+vZPYxsOisxBQoo5ISchxf2Mrxha3Mm17Ek7vS+NvuVFbXp7ByxztNlUnmyElxZCU70kKO9p4wfc7e/uoJG73Ox0n/n58iNekQZqdcz5dC93Dui7+ic9WtPJZ7EatKLiI1K5/CrDSKslNJSfJe57Kj54zwpCLBUpiMwu+fr6I3HOazJ8x9786/fgO6mrzrSizBqwDKqBWnOz5a3sVHy73O8F0dITa3JlHdnkRNRxItvUZHn9HVbzR1dJNsjiRzJBukh8KkhsKkhdzb39OSwqSFwqSGHKnmTUs/ED79zugNG93hEO39IXZkH0ZnTz+dvXn8qufL3NtxDlf03MaFzb/ntKa7ubn/g/yu70xayCYvI4Xi7DS2NbRz2Kx8ls7OY2Z+Bqb3nsSZwuQANbR1c9sL2zjzkGmUFWe9e+eGh2Dd7XDC1TDtkMQUKDExXJNYKjDXYG7We4+PpbfmTB2ypYxdnMLDzes5qPJGrqr7E/+c8hdeyD6V+5PPYk3nDH77j630R0YOZKUlM7c4iwVTslkwNYe8jHefPessRoKgMDlAP3uyko7efq4+beG7dzRXw8oveSNz3n9tYoqTCa05bwkvrvgZG1orWVT1B47f+TAnhR+mtuAIKuedz+qME9jSmsSOxg4217Xxeo03XUtJThoHT8/l0Fl5TMvVyEIJhsLkAGzY1cLtq7bzsaPnMH9Kzjs7ejrgzsu8ebguvElL8kqgmnMW8NKh32LdoquZW/MAC7bfxfEV/8kxoVRqSk5i28wz2Ln0WGo6UqisbeXNPa08W1nH396sozg7jT0tXZx92AwWTcsZ+cVEoqQwiVJXbz9fvWsdBZmpfOUDg85K+nvhnk/Brte89SqKFySsRplcelLz2Fh+ORvLPklR02uU7XqEObseZc6exwlbiNbMUpqyF9A0dQG106ewqimHF/bm8IunuvifpzazILeP82Z3cd7sbkqzB03nornBZBQUJlFwzvFfD61n4+5Wbv7UkRRmRc48ervg3s94F5qdcz0sOiOxhcrkZEZDwVIaCpaydvHXWbrpevJbK8lv20zpnr9SuuevdKXksyyrlEunlJF31OE80jCdP+9I4ycV2fykIpulBb2cN6eLc2d1s4/5r0X2y9wEWVdhxYoVbs2aNYE89w1Pb+ZHj23i8++by7VnHeRtbN3jTSO+/QU480dw9JW+X2fVPZpZWGIrrWcv+a2byW3fSk7HNlL6O70dGYVQNJ+9OQv4S+fB3L5rBhXNqRiOY+YWc/6yGZx5yHTyMlP2/wIykkkzrE5hsh/hsOOHj23iV8+8xXlLZ/DfH11GyPCmD3/4auhugw/90ltBMQYUJhIo58joriW3fRu57VXvCpee5Czq0kpZG17An7qP5LmOWfRZGu9fVMLJi6dw0sKS4WfGlpFMmjBRM9c+vFXXxrX3v85LWxv5+DFzuO6cJYSqnoWnvws7VsG0Q+GTN8LUgxNdqkh0zOhMn0pn+lT2FB31drjkdOwgu6Oaos5qzu1Zz7k8SH9GMltT5vPyjgX8481SbnalWNE8Tlg4jSNKCzh8dj6zCnQ9i7wj0DMTMzsD+BmQBPzWOff9IfvTgFuB5UAD8FHnXFVk37XAFUA/8GXn3GP7e61YnJn09IVZs62Ru1bv4M+v7iQnLcSPT0rlA6GXsdfvgfo3IWcGnPR1OPwTkBTbJgCdmUiiJfe105ZVSnHTOor3vsqU1gqsz7tws9vS2BieRUX/HN5yM2lMm0XWtAXkzljAnCkFzC3Jpqwok6LsNJJCCpmISfODCOzMxMySgBuA04BqYLWZrXTOrR902BXAXufcfDO7BPgB8FEzWwJcAhwMzACeMLOFzrnYrSAUDkNnI+HOZu78xxtsqd7F3toapvbv4YTkBr5S3ERp9yZCz7R6x5ce703Ct/RSzQIsE1ZfchY1U0+mZurJAFi4l7y2LRS0bqKgZSNTWzaysOVlMvqehjCwE8I1xk6KqHX5vOLyqSePtpQietMKsbQcwinZhFOzcKnZuNRsLCUDZ8lsquuiz0L0u2T6LIl+kuh3oXfNHGHmzWO2ZHouSSHDzEgKQZIZyUkhUpJCpCRZ5Lt3OzU59O77SSFSkvdx7MDt5Mi+UIiQgnBUgmzmOgrY7JzbAmBmdwLnA4PD5Hzgusjte4FfmHfefD5wp3OuG9hqZpsjz/dCzKrraoIfzSMEXDawLeR9ucxiLLcUpl0Ec46BshMhb2bMXlpkvHChFJpyF9GUu4itM8+LbHSk9jaT07GdnPbtZHVsJ61lO3mdtZS7JtK6NpPZ1wSdeF8HqM+FcNjbX78IX8hNOy4g7BzhMPQ79/bV/kFICnkBZhiR/2EGpy2Zxs8vPTyw1x3vggyTmcCOQfergaP3dYxzrs/MmoGiyPYXhzz2PZ/mZnYlMDCMqs3MNh1gjcVA/Xs3twBbgKfxWukCtY8a4ko1qIZBvjYGahj8c/h15CuRNcAm4BeX7fvgfXjUOTcprhkY1x3wzjlf7zIzW+OcWxHDklSDalANqmFSCnJxrBpg8MpRsyLbhj3GzJKBPLyO+GgeKyIiY0SQYbIaWGBm5WaWitehvnLIMSuByyO3LwKect7wspXAJWaWZmblwALgpQBrFRERHwJr5or0gXwReAxvaPDvnHMVZvZtYI1zbiVwE3BbpIO9ES9wiBx3N15nfR/whZiO5HpHQhpih1ANHtXgUQ0e1TDOTJgr4EVEJHGCbOYSEZFJQmEiIiK+TZowMbPfmVmtmb0xaFuhmT1uZpWR7wUBvv5sM3vazNabWYWZXZWAGtLN7CUzezVSw7ci28vNbJWZbTazuyIDJgJlZklm9oqZPZTAGqrM7HUzW2dmayLb4vb7iLxevpnda2YbzWyDmR0b5/fEosi/f+Crxcy+koCfw1cj78k3zOyPkfdqXN8TZnZV5PUrzOwrkW1x/TmMZ5MmTIDfA0MvHroGeNI5twB4MnI/KH3AvzrnlgDHAF+ITBsTzxq6gVOcc0uBZcAZZnYM3jQ21zvn5gN78aa5CdpVwIZB9xNRA8DJzrllg64niOfvA7yrYh91zi0GluL9TOJWg3NuU+TfvwxvjrwO4E/xrMHMZgJfBlY45w7BG7AzML1SXN4TZnYI8Dm8mTaWAueY2Xzi/34Yv5xzk+YLKAPeGHR/EzA9cns6sCmOtTyIN29ZQmoAMoG1eLMS1APJke3HAo8F/Nqz8P7DPAV4CG/GirjWEHmdKqB4yLa4/T7wrqvaSmQgTCJqGPK6pwPPJeDnMDATRiHeCNOHgA/G8z0BXAzcNOj+fwD/lsjPiPH2NZnOTIYz1Tm3K3J7NzA1Hi9qZmXA4cCqeNcQaV5aB9QCjwNvAU3Oub7IIcNOXRNj/433H2o4cr8oATUAOOCvZvZyZGoeiO/voxyoA26ONPn91syy4lzDYJcAf4zcjlsNzrka4MfAdmAX0Ay8THzfE28AJ5pZkZllAmfhXTidqN/FuDPZw+RtzvvTI/Bx0maWDdwHfMU51xLvGpxz/c5r0piFd0q/OMjXG8rMzgFqnXMvx/N19+EE59wRwJl4zY7vG7wzDr+PZOAI4H+dc4cD7QxpRonj+zIVOA+4Z+i+oGuI9EOcjxeuM4As3tskHSjn3Aa8ZrW/Ao8C6/CWvxh8TFx+F+PVZA+TPWY2HSDyvTbIFzOzFLwgud05d38iahjgnGvCm8nyWCA/Mp0NBD91zfHAeWZWBdyJ19T1szjXALz9FzHOuVq8foKjiO/voxqods6tity/Fy9cEvGeOBNY65zbE7kfzxo+AGx1ztU553qB+/HeJ3F9TzjnbnLOLXfOvQ+vj+ZNEvTf53g02cNk8HQul+P1YwTCzAzviv8NzrmfJqiGEjPLj9zOwOuz2YAXKhfFowbn3LXOuVnOuTK8ZpWnnHMfi2cNAGaWZWY5A7fx+gveII6/D+fcbmCHmS2KbDoVb9aHuNUwyKW808RFnGvYDhxjZpmR/04Gfg7xfk9MiXyfA3wYuIPE/C7Gp0R32sTrC+8/lF1AL95fhFfgtdU/CVQCTwCFAb7+CXinyK/hnUKvw2uXjWcNhwGvRGp4A/hmZPtcvLnPNuM1c6TF6XfyfuChRNQQeb1XI18VwP+NbI/b7yPyesuANZHfyQNAQQJqyMKbYDVv0LZ41/AtYGPkfXkbkJaA98Tf8ULsVeDURPwcxvOXplMRERHfJnszl4iIxIDCREREfFOYiIiIbwoTERHxTWEiIiK+KUxkUjKzD5mZM7O4zgAgMlEpTGSyuhT4R+S7iPikMJFJJzI/2gl4F65eEtkWMrNfRtYVedzMHjGziyL7lpvZM5EJIR8bmF5DRN6hMJHJ6Hy8NUTeBBrMbDne9BllwBLgE3hzlg3Mp/Zz4CLn3HLgd8B3E1G0yFiWPPIhIhPOpXiTS4I32eSleP8t3OOcCwO7zezpyP5FwCHA4960USThTcsjIoMoTGRSMbNCvJmKDzUzhxcODm/W4GEfAlQ4546NU4ki45KauWSyuQi4zTlX6pwrc87NxlvtsBG4MNJ3MhVvEkrwVtorMbO3m73M7OBEFC4ylilMZLK5lPeehdwHTMObTXo98Ae8JY2bnXM9eAH0AzN7FW+25+PiVq3IOKFZg0UizCzbOddmZkV4U58f77w1R0RkBOozEXnHQ5HFw1KB/1KQiERPZyYiIuKb+kxERMQ3hYmIiPimMBEREd8UJiIi4pvCREREfPv/Uq8sSGGm82QAAAAASUVORK5CYII=\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":"sns.heatmap(df.corr())","metadata":{"execution":{"iopub.status.busy":"2023-01-17T16:11:09.416017Z","iopub.execute_input":"2023-01-17T16:11:09.416697Z","iopub.status.idle":"2023-01-17T16:11:09.761146Z","shell.execute_reply.started":"2023-01-17T16:11:09.416661Z","shell.execute_reply":"2023-01-17T16:11:09.759913Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"<AxesSubplot:>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 2 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":"X= df[df.columns.difference(['Outcome'])].copy()\nX.head()","metadata":{"execution":{"iopub.status.busy":"2023-01-17T16:11:09.806279Z","iopub.execute_input":"2023-01-17T16:11:09.806726Z","iopub.status.idle":"2023-01-17T16:11:09.823426Z","shell.execute_reply.started":"2023-01-17T16:11:09.806691Z","shell.execute_reply":"2023-01-17T16:11:09.821904Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"   Age   BMI  BloodPressure  DiabetesPedigreeFunction  Glucose  Insulin  \\\n0   50  33.6             72                     0.627      148        0   \n1   31  26.6             66                     0.351       85        0   \n2   32  23.3             64                     0.672      183        0   \n3   21  28.1             66                     0.167       89       94   \n4   33  43.1             40                     2.288      137      168   \n\n   Pregnancies  SkinThickness  \n0            6             35  \n1            1             29  \n2            8              0  \n3            1             23  \n4            0             35  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Age</th>\n      <th>BMI</th>\n      <th>BloodPressure</th>\n      <th>DiabetesPedigreeFunction</th>\n      <th>Glucose</th>\n      <th>Insulin</th>\n      <th>Pregnancies</th>\n      <th>SkinThickness</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>50</td>\n      <td>33.6</td>\n      <td>72</td>\n      <td>0.627</td>\n      <td>148</td>\n      <td>0</td>\n      <td>6</td>\n      <td>35</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>31</td>\n      <td>26.6</td>\n      <td>66</td>\n      <td>0.351</td>\n      <td>85</td>\n      <td>0</td>\n      <td>1</td>\n      <td>29</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>32</td>\n      <td>23.3</td>\n      <td>64</td>\n      <td>0.672</td>\n      <td>183</td>\n      <td>0</td>\n      <td>8</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>21</td>\n      <td>28.1</td>\n      <td>66</td>\n      <td>0.167</td>\n      <td>89</td>\n      <td>94</td>\n      <td>1</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>33</td>\n      <td>43.1</td>\n      <td>40</td>\n      <td>2.288</td>\n      <td>137</td>\n      <td>168</td>\n      <td>0</td>\n      <td>35</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrainx , testx , trainy , testy = train_test_split(X , df['Outcome'] , random_state = 0 , test_size = 0.25)","metadata":{"execution":{"iopub.status.busy":"2023-01-17T16:11:10.535942Z","iopub.execute_input":"2023-01-17T16:11:10.536364Z","iopub.status.idle":"2023-01-17T16:11:10.544494Z","shell.execute_reply.started":"2023-01-17T16:11:10.536328Z","shell.execute_reply":"2023-01-17T16:11:10.543320Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"print(trainx.shape)\nprint(testx.shape)\nprint(trainy.shape)\nprint(testy.shape)","metadata":{"execution":{"iopub.status.busy":"2023-01-17T16:11:11.045186Z","iopub.execute_input":"2023-01-17T16:11:11.046244Z","iopub.status.idle":"2023-01-17T16:11:11.052337Z","shell.execute_reply.started":"2023-01-17T16:11:11.046204Z","shell.execute_reply":"2023-01-17T16:11:11.051265Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"(576, 8)\n(192, 8)\n(576,)\n(192,)\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader , Dataset\nimport torch.nn.functional as F","metadata":{"execution":{"iopub.status.busy":"2023-01-17T16:11:11.356681Z","iopub.execute_input":"2023-01-17T16:11:11.357620Z","iopub.status.idle":"2023-01-17T16:11:13.032878Z","shell.execute_reply.started":"2023-01-17T16:11:11.357553Z","shell.execute_reply":"2023-01-17T16:11:13.031767Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"##Dataloader\nclass train_loader(Dataset):\n    def __init__(self):\n        self.x = torch.from_numpy(trainx.values.astype(np.float32))\n        self.y = torch.from_numpy(trainy.values.astype(np.float32))\n        self.n_samples = self.x.shape[0]\n        \n    def __getitem__(self , index):\n        return self.x[index] , self.y[index]\n            \n    def __len__(self):\n        return self.n_samples\n\nclass val_loader(Dataset):\n    def __init__(self):\n        self.x = torch.from_numpy(testx.values.astype(np.float32))\n        self.y =  torch.from_numpy(testy.values.astype(np.float32))\n        \n    def __getitem__(self , index):\n        return self.x[index] , self.y[index]\n    \n    def __len__(self):\n        return self.x.shape[0]\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2023-01-17T16:11:13.034806Z","iopub.execute_input":"2023-01-17T16:11:13.035411Z","iopub.status.idle":"2023-01-17T16:11:13.044693Z","shell.execute_reply.started":"2023-01-17T16:11:13.035377Z","shell.execute_reply":"2023-01-17T16:11:13.043750Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"train = train_loader()\ntraindataloader = DataLoader(train , batch_size = 16 )","metadata":{"execution":{"iopub.status.busy":"2023-01-17T16:11:13.045845Z","iopub.execute_input":"2023-01-17T16:11:13.046842Z","iopub.status.idle":"2023-01-17T16:11:13.060319Z","shell.execute_reply.started":"2023-01-17T16:11:13.046805Z","shell.execute_reply":"2023-01-17T16:11:13.058964Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"len(traindataloader)","metadata":{"execution":{"iopub.status.busy":"2023-01-17T16:11:14.883273Z","iopub.execute_input":"2023-01-17T16:11:14.883737Z","iopub.status.idle":"2023-01-17T16:11:14.891450Z","shell.execute_reply.started":"2023-01-17T16:11:14.883696Z","shell.execute_reply":"2023-01-17T16:11:14.890071Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"36"},"metadata":{}}]},{"cell_type":"code","source":"val = val_loader()\nvaldataloader = DataLoader(val , batch_size = 1)","metadata":{"execution":{"iopub.status.busy":"2023-01-17T16:11:15.653971Z","iopub.execute_input":"2023-01-17T16:11:15.654408Z","iopub.status.idle":"2023-01-17T16:11:15.660527Z","shell.execute_reply.started":"2023-01-17T16:11:15.654373Z","shell.execute_reply":"2023-01-17T16:11:15.659683Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"## model\nclass neural_net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear1 = nn.Linear(8 , 64)\n        self.linear2 = nn.Linear(64 , 32)\n        self.linear3 = nn.Linear(32 , 16)\n        self.linear4 = nn.Linear(16 , 1)\n        self.dropout =  nn.Dropout(0.2)\n    def forward(self , x):\n        x = F.relu(self.linear1(x))\n        x = self.dropout(x)\n        x =  F.relu(self.linear2(x))\n        x = self.dropout(x)\n        x = F.relu(self.linear3(x))\n        x = self.dropout(x)\n        x = F.relu(self.linear4(x))\n        x = torch.sigmoid(x)\n        return x\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2023-01-17T16:11:16.164358Z","iopub.execute_input":"2023-01-17T16:11:16.165564Z","iopub.status.idle":"2023-01-17T16:11:16.174312Z","shell.execute_reply.started":"2023-01-17T16:11:16.165514Z","shell.execute_reply":"2023-01-17T16:11:16.173197Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"torch.manual_seed(0)\nmodel = neural_net()","metadata":{"execution":{"iopub.status.busy":"2023-01-17T16:11:17.427617Z","iopub.execute_input":"2023-01-17T16:11:17.428369Z","iopub.status.idle":"2023-01-17T16:11:17.444560Z","shell.execute_reply.started":"2023-01-17T16:11:17.428325Z","shell.execute_reply":"2023-01-17T16:11:17.443655Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"## Early Stopping\nclass EarlyStopping:\n    \n    def __init__(self, patience=7, verbose=False, delta=0, path='/kaggle/working/checkpoint.pt', trace_func=print):\n                 \n      \n        self.patience = patience\n        self.verbose = verbose\n        self.counter = 0\n        self.best_score = None\n        self.early_stop = False\n        self.val_loss_min = np.Inf\n        self.delta = delta\n        self.path = path\n        self.trace_func = trace_func\n    def __call__(self, val_loss, model):\n\n        score = -val_loss\n\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n        elif score < self.best_score + self.delta:\n            self.counter += 1\n            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n            self.counter = 0\n\n    def save_checkpoint(self, val_loss, model):\n        '''Saves model when validation loss decrease.'''\n        if self.verbose:\n            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n        torch.save(model.state_dict(), self.path)\n        self.val_loss_min = val_loss","metadata":{"execution":{"iopub.status.busy":"2023-01-17T16:11:18.589850Z","iopub.execute_input":"2023-01-17T16:11:18.590695Z","iopub.status.idle":"2023-01-17T16:11:18.603463Z","shell.execute_reply.started":"2023-01-17T16:11:18.590656Z","shell.execute_reply":"2023-01-17T16:11:18.602009Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"model.parameters","metadata":{"execution":{"iopub.status.busy":"2023-01-17T16:11:19.597924Z","iopub.execute_input":"2023-01-17T16:11:19.598375Z","iopub.status.idle":"2023-01-17T16:11:19.606344Z","shell.execute_reply.started":"2023-01-17T16:11:19.598332Z","shell.execute_reply":"2023-01-17T16:11:19.605010Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"<bound method Module.parameters of neural_net(\n  (linear1): Linear(in_features=8, out_features=64, bias=True)\n  (linear2): Linear(in_features=64, out_features=32, bias=True)\n  (linear3): Linear(in_features=32, out_features=16, bias=True)\n  (linear4): Linear(in_features=16, out_features=1, bias=True)\n  (dropout): Dropout(p=0.2, inplace=False)\n)>"},"metadata":{}}]},{"cell_type":"code","source":"num_epochs = 10000\nloss_fn = nn.BCELoss()\noptimizer = torch.optim.SGD(model.parameters() , lr = 0.01)\n","metadata":{"execution":{"iopub.status.busy":"2023-01-17T16:11:20.324795Z","iopub.execute_input":"2023-01-17T16:11:20.325236Z","iopub.status.idle":"2023-01-17T16:11:20.332156Z","shell.execute_reply.started":"2023-01-17T16:11:20.325198Z","shell.execute_reply":"2023-01-17T16:11:20.330678Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"## training\ndef training(model , patience , n_epochs):\n    avg_train_loss = []\n    avg_val_loss = []\n    train_loss = []\n    val_loss = []\n    \n    early_stop = EarlyStopping(patience = patience  , verbose = True)\n    for epoch in range(num_epochs):\n        model.train()\n        for i , (x , label) in enumerate(traindataloader):\n            label = label.unsqueeze(1)\n            label = label.float()\n            y_pred = model(x)\n            loss = loss_fn(y_pred , label)\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n            train_loss.append(loss.item())\n        model.eval()\n        for i , (x ,label) in enumerate(valdataloader):\n            label = label.unsqueeze(1)\n            label = label.float()\n            y_pred = model(x)\n            loss = loss_fn(y_pred , label)\n            val_loss.append(loss.item())\n            \n        val_l = np.average(val_loss)\n        train_l  = np.average(train_loss)\n        \n        avg_val_loss.append(val_l)\n        avg_train_loss.append(train_l)\n        print('Epoch : {} , train loss {} , val loss {}'.format(epoch , train_l , val_l))\n        train_loss = []\n        val_loss = []\n        \n        early_stop(val_l , model)\n        if early_stop.early_stop:\n            print('Stopping')\n            break\n            \n    model.load_state_dict(torch.load('/kaggle/working/checkpoint.pt'))\n    return model , avg_val_loss , avg_train_loss\n        \n    \n    \n            ","metadata":{"execution":{"iopub.status.busy":"2023-01-17T16:11:22.279080Z","iopub.execute_input":"2023-01-17T16:11:22.279841Z","iopub.status.idle":"2023-01-17T16:11:22.292503Z","shell.execute_reply.started":"2023-01-17T16:11:22.279803Z","shell.execute_reply":"2023-01-17T16:11:22.291005Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"model , valid_loss , train_loss = training(model , 20 , num_epochs)","metadata":{"execution":{"iopub.status.busy":"2023-01-17T16:11:23.283842Z","iopub.execute_input":"2023-01-17T16:11:23.284270Z","iopub.status.idle":"2023-01-17T16:14:29.610619Z","shell.execute_reply.started":"2023-01-17T16:11:23.284234Z","shell.execute_reply":"2023-01-17T16:14:29.609332Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"Epoch : 0 , train loss 0.7302504231532415 , val loss 0.6931471824645996\nValidation loss decreased (inf --> 0.693147).  Saving model ...\nEpoch : 1 , train loss 0.693208826912774 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2 , train loss 0.6972145752774345 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 3 , train loss 0.6928308556477228 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 4 , train loss 0.693341538310051 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 5 , train loss 0.6934419473012289 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 6 , train loss 0.6928981393575668 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 7 , train loss 0.6939908034271665 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 8 , train loss 0.6937031431330575 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 9 , train loss 0.6936330431037478 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 10 , train loss 0.6930563482973311 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 11 , train loss 0.6925681183735529 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 12 , train loss 0.6939126468367047 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 13 , train loss 0.6927068514956368 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 14 , train loss 0.6936084065172408 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 15 , train loss 0.6928305145767 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 16 , train loss 0.6939452836910883 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 17 , train loss 0.6928816139698029 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 18 , train loss 0.6931470930576324 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 19 , train loss 0.6947258296940062 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 20 , train loss 0.6926217244731055 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 21 , train loss 0.6935135606262419 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 22 , train loss 0.6934168802367316 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 23 , train loss 0.6933355712228351 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 24 , train loss 0.6922844250996908 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 25 , train loss 0.6930192543400658 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 26 , train loss 0.6926280160744985 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 27 , train loss 0.692589185304112 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 28 , train loss 0.6947556138038635 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 29 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 30 , train loss 0.6932917982339859 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 31 , train loss 0.6933312979009416 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 32 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 33 , train loss 0.6927597737974591 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 34 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 35 , train loss 0.6928422186109755 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 36 , train loss 0.693335720234447 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 37 , train loss 0.6934041165643268 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 38 , train loss 0.6932280626561906 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 39 , train loss 0.6928791138860915 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 40 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 41 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 42 , train loss 0.6928802380959193 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 43 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 44 , train loss 0.6932262397474713 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 45 , train loss 0.693461020787557 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 46 , train loss 0.6937020123004913 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 47 , train loss 0.6932843029499054 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 48 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 49 , train loss 0.69268270333608 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 50 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 51 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 52 , train loss 0.6929285890526242 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 53 , train loss 0.6933809204234017 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 54 , train loss 0.6932174033588834 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 55 , train loss 0.6922048280636469 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 56 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 57 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 58 , train loss 0.6928163005246056 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 59 , train loss 0.6933951394425498 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 60 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 61 , train loss 0.6934110853407118 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 62 , train loss 0.6925025582313538 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 63 , train loss 0.6938848495483398 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 64 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 65 , train loss 0.693333034714063 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 66 , train loss 0.6922371205356386 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 67 , train loss 0.6931767397456698 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 68 , train loss 0.6935688737365935 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 69 , train loss 0.6929180771112442 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 70 , train loss 0.6929323756032519 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 71 , train loss 0.6932910746998258 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 72 , train loss 0.6930764466524124 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 73 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 74 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 75 , train loss 0.694475895828671 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 76 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 77 , train loss 0.6933734930223889 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 78 , train loss 0.6932162973615859 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 79 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 80 , train loss 0.6931222213639153 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 81 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 82 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 83 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 84 , train loss 0.6926645537217458 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 85 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 86 , train loss 0.6931770410802629 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 87 , train loss 0.693109499083625 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 88 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 89 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 90 , train loss 0.6936748292711046 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 91 , train loss 0.6930507073799769 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 92 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 93 , train loss 0.6933778292602963 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 94 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 95 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 96 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 97 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 98 , train loss 0.6931857135560777 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 99 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 100 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 101 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 102 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 103 , train loss 0.6930491692490048 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 104 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 105 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 106 , train loss 0.6932869702577591 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 107 , train loss 0.6935008946392272 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 108 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 109 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 110 , train loss 0.6928887201680077 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 111 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 112 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 113 , train loss 0.6929666101932526 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 114 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 115 , train loss 0.6939208739333682 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 116 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 117 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 118 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 119 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 120 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 121 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 122 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 123 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 124 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 125 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 126 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 127 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 128 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 129 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 130 , train loss 0.6933236569166183 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 131 , train loss 0.6931735244062212 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 132 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 133 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 134 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 135 , train loss 0.693100545141432 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 136 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 137 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 138 , train loss 0.6928248322672315 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 139 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 140 , train loss 0.6931653850608401 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 141 , train loss 0.6931276602877511 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 142 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 143 , train loss 0.6930611613723967 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 144 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 145 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 146 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 147 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 148 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 149 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 150 , train loss 0.6928275922934214 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 151 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 152 , train loss 0.6931316902240118 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 153 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 154 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 155 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 156 , train loss 0.6929339286353853 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 157 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 158 , train loss 0.6933077888356315 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 159 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 160 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 161 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 162 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 163 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 164 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 165 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 166 , train loss 0.6932833989461263 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 167 , train loss 0.6930181466870837 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 168 , train loss 0.693764810760816 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 169 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 170 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 171 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 172 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 173 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 174 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 175 , train loss 0.6930908080604341 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 176 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 177 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 178 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 179 , train loss 0.6929035120540195 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 180 , train loss 0.6936822318368487 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 181 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 182 , train loss 0.6931018812788857 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 183 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 184 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 185 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 186 , train loss 0.6933346804645326 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 187 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 188 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 189 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 190 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 191 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 192 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 193 , train loss 0.6930994888146719 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 194 , train loss 0.6930591993861728 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 195 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 196 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 197 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 198 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 199 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 200 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 201 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 202 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 203 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 204 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 205 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 206 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 207 , train loss 0.6931553648577796 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 208 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 209 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 210 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 211 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 212 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 213 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 214 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 215 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 216 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 217 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 218 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 219 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 220 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 221 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 222 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 223 , train loss 0.6928825047281053 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 224 , train loss 0.6921389400959015 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 225 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 226 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 227 , train loss 0.6932036330302557 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 228 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 229 , train loss 0.6933080421553718 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 230 , train loss 0.692945228682624 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 231 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 232 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 233 , train loss 0.6926427748468187 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 234 , train loss 0.6930485434002347 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 235 , train loss 0.6932153469986386 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 236 , train loss 0.6927361107534833 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 237 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 238 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 239 , train loss 0.693115128411187 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 240 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 241 , train loss 0.6928513066636192 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 242 , train loss 0.6923943178521262 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 243 , train loss 0.6927174412541919 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 244 , train loss 0.6934889157613119 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 245 , train loss 0.6932605422205396 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 246 , train loss 0.6921532385879092 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 247 , train loss 0.6930703388320075 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 248 , train loss 0.6930295907788806 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 249 , train loss 0.6931767728593614 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 250 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 251 , train loss 0.6912833154201508 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 252 , train loss 0.6931290924549103 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 253 , train loss 0.6911647700601153 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 254 , train loss 0.6949367473522822 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 255 , train loss 0.6931585222482681 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 256 , train loss 0.692684585849444 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 257 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 258 , train loss 0.6923332793845071 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 259 , train loss 0.693211242556572 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 260 , train loss 0.6935775296555625 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 261 , train loss 0.6936063634024726 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 262 , train loss 0.6939877602789137 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 263 , train loss 0.6935446179575391 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 264 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 265 , train loss 0.6941404806243049 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 266 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 267 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 268 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 269 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 270 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 271 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 272 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 273 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 274 , train loss 0.6933702710602019 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 275 , train loss 0.6935659117168851 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 276 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 277 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 278 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 279 , train loss 0.6929303159316381 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 280 , train loss 0.6935030553076003 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 281 , train loss 0.6932959523465898 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 282 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 283 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 284 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 285 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 286 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 287 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 288 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 289 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 290 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 291 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 292 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 293 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 294 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 295 , train loss 0.6940594779120551 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 296 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 297 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 298 , train loss 0.6937384522623486 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 299 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 300 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 301 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 302 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 303 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 304 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 305 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 306 , train loss 0.6930374801158905 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 307 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 308 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 309 , train loss 0.693174218138059 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 310 , train loss 0.6950719720787473 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 311 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 312 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 313 , train loss 0.6924049390686883 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 314 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 315 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 316 , train loss 0.6931628866328133 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 317 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 318 , train loss 0.6930814501312044 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 319 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 320 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 321 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 322 , train loss 0.6932556132475535 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 323 , train loss 0.6925342397557365 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 324 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 325 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 326 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 327 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 328 , train loss 0.6931056479612986 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 329 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 330 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 331 , train loss 0.69286700255341 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 332 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 333 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 334 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 335 , train loss 0.6938360085090002 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 336 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 337 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 338 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 339 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 340 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 341 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 342 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 343 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 344 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 345 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 346 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 347 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 348 , train loss 0.6930873394012451 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 349 , train loss 0.6932174795203738 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 350 , train loss 0.692769017484453 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 351 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 352 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 353 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 354 , train loss 0.6927871604760488 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 355 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 356 , train loss 0.6931497504313787 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 357 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 358 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 359 , train loss 0.6929863641659418 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 360 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 361 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 362 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 363 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 364 , train loss 0.6924916075335609 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 365 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 366 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 367 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 368 , train loss 0.6932992471588982 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 369 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 370 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 371 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 372 , train loss 0.6927489952908622 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 373 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 374 , train loss 0.6932108932071261 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 375 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 376 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 377 , train loss 0.6930522157086266 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 378 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 379 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 380 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 381 , train loss 0.6932482934660382 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 382 , train loss 0.6931592490937974 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 383 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 384 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 385 , train loss 0.6935548219415877 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 386 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 387 , train loss 0.6941426777177386 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 388 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 389 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 390 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 391 , train loss 0.693593344754643 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 392 , train loss 0.6932249085770713 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 393 , train loss 0.6926480117771361 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 394 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 395 , train loss 0.6933688471714655 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 396 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 397 , train loss 0.6931607955031924 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 398 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 399 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 400 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 401 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 402 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 403 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 404 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 405 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 406 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 407 , train loss 0.6944130874342389 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 408 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 409 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 410 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 411 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 412 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 413 , train loss 0.6930541843175888 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 414 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 415 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 416 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 417 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 418 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 419 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 420 , train loss 0.6932441492875417 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 421 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 422 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 423 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 424 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 425 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 426 , train loss 0.6924380030896928 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 427 , train loss 0.6931721816460291 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 428 , train loss 0.6938483417034149 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 429 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 430 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 431 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 432 , train loss 0.6932173553440306 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 433 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 434 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 435 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 436 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 437 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 438 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 439 , train loss 0.6927162839306725 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 440 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 441 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 442 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 443 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 444 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 445 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 446 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 447 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 448 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 449 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 450 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 451 , train loss 0.6934663835499022 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 452 , train loss 0.6936507787969377 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 453 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 454 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 455 , train loss 0.6931556711594263 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 456 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 457 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 458 , train loss 0.6924757212400436 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 459 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 460 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 461 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 462 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 463 , train loss 0.6928721517324448 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 464 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 465 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 466 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 467 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 468 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 469 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 470 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 471 , train loss 0.6931119246615304 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 472 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 473 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 474 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 475 , train loss 0.6932722247309155 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 476 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 477 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 478 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 479 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 480 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 481 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 482 , train loss 0.6931488282150693 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 483 , train loss 0.6929001675711738 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 484 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 485 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 486 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 487 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 488 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 489 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 490 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 491 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 492 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 493 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 494 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 495 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 496 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 497 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 498 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 499 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 500 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 501 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 502 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 503 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 504 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 505 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 506 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 507 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 508 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 509 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 510 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 511 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 512 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 513 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 514 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 515 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 516 , train loss 0.6929495334625244 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 517 , train loss 0.6937036746078067 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 518 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 519 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 520 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 521 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 522 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 523 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 524 , train loss 0.6931065436866548 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 525 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 526 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 527 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 528 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 529 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 530 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 531 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 532 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 533 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 534 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 535 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 536 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 537 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 538 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 539 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 540 , train loss 0.6929452121257782 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 541 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 542 , train loss 0.6927251782682207 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 543 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 544 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 545 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 546 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 547 , train loss 0.6928279101848602 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 548 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 549 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 550 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 551 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 552 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 553 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 554 , train loss 0.6930447436041303 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 555 , train loss 0.692760662900077 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 556 , train loss 0.6929724911848704 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 557 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 558 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 559 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 560 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 561 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 562 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 563 , train loss 0.6934554328521093 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 564 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 565 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 566 , train loss 0.692958234084977 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 567 , train loss 0.6923018114434348 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 568 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 569 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 570 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 571 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 572 , train loss 0.6925211317009397 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 573 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 574 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 575 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 576 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 577 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 578 , train loss 0.6929449604617225 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 579 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 580 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 581 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 582 , train loss 0.6934104975726869 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 583 , train loss 0.6935045520464579 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 584 , train loss 0.6936848527855344 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 585 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 586 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 587 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 588 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 589 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 590 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 591 , train loss 0.693190841211213 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 592 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 593 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 594 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 595 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 596 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 597 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 598 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 599 , train loss 0.692979516254531 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 600 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 601 , train loss 0.6930062257581286 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 602 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 603 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 604 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 605 , train loss 0.6932631184657415 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 606 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 607 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 608 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 609 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 610 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 611 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 612 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 613 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 614 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 615 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 616 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 617 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 618 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 619 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 620 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 621 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 622 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 623 , train loss 0.6930691334936354 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 624 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 625 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 626 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 627 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 628 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 629 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 630 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 631 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 632 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 633 , train loss 0.6930791437625885 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 634 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 635 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 636 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 637 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 638 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 639 , train loss 0.6931843178139793 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 640 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 641 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 642 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 643 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 644 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 645 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 646 , train loss 0.6930629329548942 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 647 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 648 , train loss 0.6930420713292228 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 649 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 650 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 651 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 652 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 653 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 654 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 655 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 656 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 657 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 658 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 659 , train loss 0.6930903229448531 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 660 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 661 , train loss 0.692689006527265 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 662 , train loss 0.6932071778509352 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 663 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 664 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 665 , train loss 0.6930936012003157 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 666 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 667 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 668 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 669 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 670 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 671 , train loss 0.6931782017151514 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 672 , train loss 0.6938738425572714 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 673 , train loss 0.6931616630819109 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 674 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 675 , train loss 0.6933918976121478 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 676 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 677 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 678 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 679 , train loss 0.693274931775199 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 680 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 681 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 682 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 683 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 684 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 685 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 686 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 687 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 688 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 689 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 690 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 691 , train loss 0.6932230227523379 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 692 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 693 , train loss 0.6932568616337247 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 694 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 695 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 696 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 697 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 698 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 699 , train loss 0.6933731155263053 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 700 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 701 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 702 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 703 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 704 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 705 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 706 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 707 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 708 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 709 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 710 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 711 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 712 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 713 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 714 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 715 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 716 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 717 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 718 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 719 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 720 , train loss 0.6931926939222548 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 721 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 722 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 723 , train loss 0.6930083649026023 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 724 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 725 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 726 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 727 , train loss 0.6930492884582944 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 728 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 729 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 730 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 731 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 732 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 733 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 734 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 735 , train loss 0.6931417600976096 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 736 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 737 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 738 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 739 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 740 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 741 , train loss 0.6932936211427053 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 742 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 743 , train loss 0.692849240369267 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 744 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 745 , train loss 0.693134488330947 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 746 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 747 , train loss 0.693157563606898 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 748 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 749 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 750 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 751 , train loss 0.6929443147447374 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 752 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 753 , train loss 0.6929811437924703 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 754 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 755 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 756 , train loss 0.6932313210434384 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 757 , train loss 0.6930817994806502 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 758 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 759 , train loss 0.6942031747765012 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 760 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 761 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 762 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 763 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 764 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 765 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 766 , train loss 0.6931764284769694 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 767 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 768 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 769 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 770 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 771 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 772 , train loss 0.693470858865314 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 773 , train loss 0.6931673702266481 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 774 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 775 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 776 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 777 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 778 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 779 , train loss 0.6933925019370185 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 780 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 781 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 782 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 783 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 784 , train loss 0.6931700557470322 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 785 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 786 , train loss 0.6935300578673681 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 787 , train loss 0.6930490467283461 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 788 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 789 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 790 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 791 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 792 , train loss 0.6928488959868749 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 793 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 794 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 795 , train loss 0.6930026080873277 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 796 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 797 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 798 , train loss 0.6931498216258155 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 799 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 800 , train loss 0.693043879336781 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 801 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 802 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 803 , train loss 0.6925320128599802 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 804 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 805 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 806 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 807 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 808 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 809 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 810 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 811 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 812 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 813 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 814 , train loss 0.6932085023985969 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 815 , train loss 0.6936681585179435 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 816 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 817 , train loss 0.6924684461620119 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 818 , train loss 0.6940315183666017 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 819 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 820 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 821 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 822 , train loss 0.6931269268194834 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 823 , train loss 0.6931519955396652 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 824 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 825 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 826 , train loss 0.6927817563215891 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 827 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 828 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 829 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 830 , train loss 0.693642869591713 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 831 , train loss 0.6933446062935723 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 832 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 833 , train loss 0.6931454324060016 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 834 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 835 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 836 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 837 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 838 , train loss 0.6934297879536947 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 839 , train loss 0.6930916408697764 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 840 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 841 , train loss 0.6922741201188829 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 842 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 843 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 844 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 845 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 846 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 847 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 848 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 849 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 850 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 851 , train loss 0.6935832103093466 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 852 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 853 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 854 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 855 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 856 , train loss 0.6937240726417966 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 857 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 858 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 859 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 860 , train loss 0.6922984139786826 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 861 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 862 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 863 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 864 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 865 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 866 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 867 , train loss 0.6929966906706492 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 868 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 869 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 870 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 871 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 872 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 873 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 874 , train loss 0.6931858758131663 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 875 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 876 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 877 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 878 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 879 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 880 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 881 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 882 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 883 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 884 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 885 , train loss 0.6929560518927045 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 886 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 887 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 888 , train loss 0.692786133951611 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 889 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 890 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 891 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 892 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 893 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 894 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 895 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 896 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 897 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 898 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 899 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 900 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 901 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 902 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 903 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 904 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 905 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 906 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 907 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 908 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 909 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 910 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 911 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 912 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 913 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 914 , train loss 0.6929994689093696 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 915 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 916 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 917 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 918 , train loss 0.6934960236152014 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 919 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 920 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 921 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 922 , train loss 0.6931711898909675 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 923 , train loss 0.6930920763148202 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 924 , train loss 0.6929854899644852 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 925 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 926 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 927 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 928 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 929 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 930 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 931 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 932 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 933 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 934 , train loss 0.6931236121389601 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 935 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 936 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 937 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 938 , train loss 0.6930076943503486 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 939 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 940 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 941 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 942 , train loss 0.6936456776327558 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 943 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 944 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 945 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 946 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 947 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 948 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 949 , train loss 0.6927424768606821 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 950 , train loss 0.6931678288512759 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 951 , train loss 0.694554939866066 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 952 , train loss 0.6926292280356089 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 953 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 954 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 955 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 956 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 957 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 958 , train loss 0.6933641168806288 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 959 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 960 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 961 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 962 , train loss 0.6921175089147356 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 963 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 964 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 965 , train loss 0.6931628104713228 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 966 , train loss 0.693198455704583 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 967 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 968 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 969 , train loss 0.6937281125121646 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 970 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 971 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 972 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 973 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 974 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 975 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 976 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 977 , train loss 0.693260489238633 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 978 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 979 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 980 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 981 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 982 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 983 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 984 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 985 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 986 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 987 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 988 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 989 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 990 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 991 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 992 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 993 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 994 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 995 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 996 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 997 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 998 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 999 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1000 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1001 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1002 , train loss 0.6930186086230807 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1003 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1004 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1005 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1006 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1007 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1008 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1009 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1010 , train loss 0.6930881076388888 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1011 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1012 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1013 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1014 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1015 , train loss 0.692296114232805 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1016 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1017 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1018 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1019 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1020 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1021 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1022 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1023 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1024 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1025 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1026 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1027 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1028 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1029 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1030 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1031 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1032 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1033 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1034 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1035 , train loss 0.6933529459767871 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1036 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1037 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1038 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1039 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1040 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1041 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1042 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1043 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1044 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1045 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1046 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1047 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1048 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1049 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1050 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1051 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1052 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1053 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1054 , train loss 0.6932711352904638 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1055 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1056 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1057 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1058 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1059 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1060 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1061 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1062 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1063 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1064 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1065 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1066 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1067 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1068 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1069 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1070 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1071 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1072 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1073 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1074 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1075 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1076 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1077 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1078 , train loss 0.6924279083808264 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1079 , train loss 0.6932496180136999 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1080 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1081 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1082 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1083 , train loss 0.6931153270933363 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1084 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1085 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1086 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1087 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1088 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1089 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1090 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1091 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1092 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1093 , train loss 0.6931896027591493 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1094 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1095 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1096 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1097 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1098 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1099 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1100 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1101 , train loss 0.6929926243093278 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1102 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1103 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1104 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1105 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1106 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1107 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1108 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1109 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1110 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1111 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1112 , train loss 0.6934705393181907 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1113 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1114 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1115 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1116 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1117 , train loss 0.6932115488582187 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1118 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1119 , train loss 0.6926828589704301 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1120 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1121 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1122 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1123 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1124 , train loss 0.6928796718517939 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1125 , train loss 0.6929836604330275 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1126 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1127 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1128 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1129 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1130 , train loss 0.6929261568519804 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1131 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1132 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1133 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1134 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1135 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1136 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1137 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1138 , train loss 0.6932324833340116 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1139 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1140 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1141 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1142 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1143 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1144 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1145 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1146 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1147 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1148 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1149 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1150 , train loss 0.6930840644571516 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1151 , train loss 0.6929957667986552 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1152 , train loss 0.6932191136810515 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1153 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1154 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1155 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1156 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1157 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1158 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1159 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1160 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1161 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1162 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1163 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1164 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1165 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1166 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1167 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1168 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1169 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1170 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1171 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1172 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1173 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1174 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1175 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1176 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1177 , train loss 0.6931473679012723 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1178 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1179 , train loss 0.6925889568196403 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1180 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1181 , train loss 0.6931250227822198 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1182 , train loss 0.6934082392189238 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1183 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1184 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1185 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1186 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1187 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1188 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1189 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1190 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1191 , train loss 0.6931592987643348 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1192 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1193 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1194 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1195 , train loss 0.6931911442014906 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1196 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1197 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1198 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1199 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1200 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1201 , train loss 0.6932328277164035 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1202 , train loss 0.6927814251846738 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1203 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1204 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1205 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1206 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1207 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1208 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1209 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1210 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1211 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1212 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1213 , train loss 0.6927575535244412 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1214 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1215 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1216 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1217 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1218 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1219 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1220 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1221 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1222 , train loss 0.693147685792711 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1223 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1224 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1225 , train loss 0.6928122424417071 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1226 , train loss 0.6928799185487959 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1227 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1228 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1229 , train loss 0.6932212511698405 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1230 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1231 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1232 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1233 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1234 , train loss 0.6931064161989424 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1235 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1236 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1237 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1238 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1239 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1240 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1241 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1242 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1243 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1244 , train loss 0.6934848676125208 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1245 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1246 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1247 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1248 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1249 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1250 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1251 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1252 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1253 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1254 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1255 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1256 , train loss 0.6930107441213396 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1257 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1258 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1259 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1260 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1261 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1262 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1263 , train loss 0.6926278753413094 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1264 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1265 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1266 , train loss 0.6945769074890349 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1267 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1268 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1269 , train loss 0.6936143851942487 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1270 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1271 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1272 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1273 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1274 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1275 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1276 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1277 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1278 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1279 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1280 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1281 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1282 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1283 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1284 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1285 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1286 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1287 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1288 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1289 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1290 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1291 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1292 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1293 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1294 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1295 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1296 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1297 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1298 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1299 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1300 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1301 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1302 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1303 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1304 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1305 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1306 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1307 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1308 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1309 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1310 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1311 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1312 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1313 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1314 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1315 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1316 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1317 , train loss 0.6935970932245255 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1318 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1319 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1320 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1321 , train loss 0.692780958281623 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1322 , train loss 0.6937008417314954 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1323 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1324 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1325 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1326 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1327 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1328 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1329 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1330 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1331 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1332 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1333 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1334 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1335 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1336 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1337 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1338 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1339 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1340 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1341 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1342 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1343 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1344 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1345 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1346 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1347 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1348 , train loss 0.6930736783477995 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1349 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1350 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1351 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1352 , train loss 0.6930348144637214 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1353 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1354 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1355 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1356 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1357 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1358 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1359 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1360 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1361 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1362 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1363 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1364 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1365 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1366 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1367 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1368 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1369 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1370 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1371 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1372 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1373 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1374 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1375 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1376 , train loss 0.6930727958679199 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1377 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1378 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1379 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1380 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1381 , train loss 0.6933797101179758 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1382 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1383 , train loss 0.6920420096980201 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1384 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1385 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1386 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1387 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1388 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1389 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1390 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1391 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1392 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1393 , train loss 0.6928451193703545 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1394 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1395 , train loss 0.6930580072932773 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1396 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1397 , train loss 0.6934467421637641 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1398 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1399 , train loss 0.6931914670599831 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1400 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1401 , train loss 0.6931175837914149 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1402 , train loss 0.6935587094889747 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1403 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1404 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1405 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1406 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1407 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1408 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1409 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1410 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1411 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1412 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1413 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1414 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1415 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1416 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1417 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1418 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1419 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1420 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1421 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1422 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1423 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1424 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1425 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1426 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1427 , train loss 0.6945617728763156 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1428 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1429 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1430 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1431 , train loss 0.693166477812661 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1432 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1433 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1434 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1435 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1436 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1437 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1438 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1439 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1440 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1441 , train loss 0.6932986146873898 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1442 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1443 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1444 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1445 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1446 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1447 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1448 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1449 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1450 , train loss 0.6934504873222775 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1451 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1452 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1453 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1454 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1455 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1456 , train loss 0.6930484970410665 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1457 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1458 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1459 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1460 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1461 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1462 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1463 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1464 , train loss 0.6928710920943154 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1465 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1466 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1467 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1468 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1469 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1470 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1471 , train loss 0.6925978230105506 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1472 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1473 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1474 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1475 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1476 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1477 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1478 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1479 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1480 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1481 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1482 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1483 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1484 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1485 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1486 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1487 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1488 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1489 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1490 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1491 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1492 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1493 , train loss 0.692946175734202 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1494 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1495 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1496 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1497 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1498 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1499 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1500 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1501 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1502 , train loss 0.6931988530688815 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1503 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1504 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1505 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1506 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1507 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1508 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1509 , train loss 0.6932732197973464 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1510 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1511 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1512 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1513 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1514 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1515 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1516 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1517 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1518 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1519 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1520 , train loss 0.6923849566115273 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1521 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1522 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1523 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1524 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1525 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1526 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1527 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1528 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1529 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1530 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1531 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1532 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1533 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1534 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1535 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1536 , train loss 0.6929982784721587 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1537 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1538 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1539 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1540 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1541 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1542 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1543 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1544 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1545 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1546 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1547 , train loss 0.6931621233622233 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1548 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1549 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1550 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1551 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1552 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1553 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1554 , train loss 0.6938380433453454 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1555 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1556 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1557 , train loss 0.6927934189637502 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1558 , train loss 0.693297372923957 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1559 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1560 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1561 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1562 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1563 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1564 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1565 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1566 , train loss 0.693576756450865 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1567 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1568 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1569 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1570 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1571 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1572 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1573 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1574 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1575 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1576 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1577 , train loss 0.6931315229998695 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1578 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1579 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1580 , train loss 0.6930131912231445 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1581 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1582 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1583 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1584 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1585 , train loss 0.6927493578857846 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1586 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1587 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1588 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1589 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1590 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1591 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1592 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1593 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1594 , train loss 0.6938495321406258 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1595 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1596 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1597 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1598 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1599 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1600 , train loss 0.6925546576579412 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1601 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1602 , train loss 0.6931752612193426 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1603 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1604 , train loss 0.6926708204878701 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1605 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1606 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1607 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1608 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1609 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1610 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1611 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1612 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1613 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1614 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1615 , train loss 0.6922277063131332 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1616 , train loss 0.6934159828556908 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1617 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1618 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1619 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1620 , train loss 0.6945397837294472 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1621 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1622 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1623 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1624 , train loss 0.6926145089997185 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1625 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1626 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1627 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1628 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1629 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1630 , train loss 0.6931533465782801 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1631 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1632 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1633 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1634 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1635 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1636 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1637 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1638 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1639 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1640 , train loss 0.6928985814253489 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1641 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1642 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1643 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1644 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1645 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1646 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1647 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1648 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1649 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1650 , train loss 0.6933960682815976 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1651 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1652 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1653 , train loss 0.6934265924824609 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1654 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1655 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1656 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1657 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1658 , train loss 0.6932633287376828 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1659 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1660 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1661 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1662 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1663 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1664 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1665 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1666 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1667 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1668 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1669 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1670 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1671 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1672 , train loss 0.6932894107368257 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1673 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1674 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1675 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1676 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1677 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1678 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1679 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1680 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1681 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1682 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1683 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1684 , train loss 0.69265106982655 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1685 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1686 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1687 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1688 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1689 , train loss 0.6931791090302997 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1690 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1691 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1692 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1693 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1694 , train loss 0.6933002521594366 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1695 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1696 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1697 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1698 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1699 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1700 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1701 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1702 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1703 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1704 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1705 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1706 , train loss 0.6933713720904456 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1707 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1708 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1709 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1710 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1711 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1712 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1713 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1714 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1715 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1716 , train loss 0.6928248918718762 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1717 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1718 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1719 , train loss 0.6930613236294852 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1720 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1721 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1722 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1723 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1724 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1725 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1726 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1727 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1728 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1729 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1730 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1731 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1732 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1733 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1734 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1735 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1736 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1737 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1738 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1739 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1740 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1741 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1742 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1743 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1744 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1745 , train loss 0.6931815031501982 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1746 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1747 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1748 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1749 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1750 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1751 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1752 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1753 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1754 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1755 , train loss 0.6925478859080209 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1756 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1757 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1758 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1759 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1760 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1761 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1762 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1763 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1764 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1765 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1766 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1767 , train loss 0.6929593020015292 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1768 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1769 , train loss 0.6931438148021698 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1770 , train loss 0.6930990964174271 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1771 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1772 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1773 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1774 , train loss 0.6931401292483012 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1775 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1776 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1777 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1778 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1779 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1780 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1781 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1782 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1783 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1784 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1785 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1786 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1787 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1788 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1789 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1790 , train loss 0.6929791851176156 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1791 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1792 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1793 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1794 , train loss 0.6931717693805695 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1795 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1796 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1797 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1798 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1799 , train loss 0.6930341985490587 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1800 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1801 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1802 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1803 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1804 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1805 , train loss 0.6927575286891725 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1806 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1807 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1808 , train loss 0.6931173619296815 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1809 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1810 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1811 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1812 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1813 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1814 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1815 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1816 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1817 , train loss 0.6925320658418868 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1818 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1819 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1820 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1821 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1822 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1823 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1824 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1825 , train loss 0.6927112738291422 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1826 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1827 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1828 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1829 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1830 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1831 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1832 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1833 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1834 , train loss 0.6946050160461001 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1835 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1836 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1837 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1838 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1839 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1840 , train loss 0.6931431508726544 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1841 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1842 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1843 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1844 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1845 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1846 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1847 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1848 , train loss 0.6927348011069827 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1849 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1850 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1851 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1852 , train loss 0.693173724744055 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1853 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1854 , train loss 0.6935246355003781 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1855 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1856 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1857 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1858 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1859 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1860 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1861 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1862 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1863 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1864 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1865 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1866 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1867 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1868 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1869 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1870 , train loss 0.6926554242769877 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1871 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1872 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1873 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1874 , train loss 0.6930200639698241 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1875 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1876 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1877 , train loss 0.6929220971133974 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1878 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1879 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1880 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1881 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1882 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1883 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1884 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1885 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1886 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1887 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1888 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1889 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1890 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1891 , train loss 0.692960113286972 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1892 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1893 , train loss 0.6927309566073947 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1894 , train loss 0.6924010068178177 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1895 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1896 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1897 , train loss 0.6930533763435152 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1898 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1899 , train loss 0.6922545085350672 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1900 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1901 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1902 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1903 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1904 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1905 , train loss 0.692556252082189 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1906 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1907 , train loss 0.6934275693363614 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1908 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1909 , train loss 0.6936323609617021 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1910 , train loss 0.6925408310360379 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1911 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1912 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1913 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1914 , train loss 0.693669800957044 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1915 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1916 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1917 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1918 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1919 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1920 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1921 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1922 , train loss 0.6940622131029764 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1923 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1924 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1925 , train loss 0.6931480864683787 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1926 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1927 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1928 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1929 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1930 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1931 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1932 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1933 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1934 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1935 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1936 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1937 , train loss 0.6923746632205116 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1938 , train loss 0.6931144661373563 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1939 , train loss 0.6932664828168021 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1940 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1941 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1942 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1943 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1944 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1945 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1946 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1947 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1948 , train loss 0.6933706584903929 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1949 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1950 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1951 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1952 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1953 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1954 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1955 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1956 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1957 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1958 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1959 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1960 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1961 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1962 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1963 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1964 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1965 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1966 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1967 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1968 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1969 , train loss 0.6928354683849547 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1970 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1971 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1972 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1973 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1974 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1975 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1976 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1977 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1978 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1979 , train loss 0.693222337298923 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1980 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1981 , train loss 0.693556214372317 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1982 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1983 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1984 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1985 , train loss 0.6929767959647708 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1986 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1987 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1988 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1989 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1990 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1991 , train loss 0.69305329852634 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1992 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1993 , train loss 0.6929687079456117 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1994 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1995 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1996 , train loss 0.6922772576411566 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1997 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1998 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 1999 , train loss 0.6926052653127246 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2000 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2001 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2002 , train loss 0.69298598004712 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2003 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2004 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2005 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2006 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2007 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2008 , train loss 0.6931637773911158 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2009 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2010 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2011 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2012 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2013 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2014 , train loss 0.6931848807467355 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2015 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2016 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2017 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2018 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2019 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2020 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2021 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2022 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2023 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2024 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2025 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2026 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2027 , train loss 0.6923002633783553 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2028 , train loss 0.6930586099624634 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2029 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2030 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2031 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2032 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2033 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2034 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2035 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2036 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2037 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2038 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2039 , train loss 0.6929241783089108 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2040 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2041 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2042 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2043 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2044 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2045 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2046 , train loss 0.6931854569249682 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2047 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2048 , train loss 0.6926747560501099 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2049 , train loss 0.6928151630693011 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2050 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2051 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2052 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2053 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2054 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2055 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2056 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2057 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2058 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2059 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2060 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2061 , train loss 0.6924671365155114 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2062 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2063 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2064 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2065 , train loss 0.6930965648757087 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2066 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2067 , train loss 0.6930625173780653 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2068 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2069 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2070 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2071 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2072 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2073 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2074 , train loss 0.6928704198863771 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2075 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2076 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2077 , train loss 0.6927763952149285 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2078 , train loss 0.6929274251063665 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2079 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2080 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2081 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2082 , train loss 0.6931555883751975 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2083 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2084 , train loss 0.6926904618740082 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2085 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2086 , train loss 0.692490408817927 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2087 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2088 , train loss 0.6928617821799384 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2089 , train loss 0.6930871920453178 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2090 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2091 , train loss 0.6927826321787305 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2092 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2093 , train loss 0.6934299154414071 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2094 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2095 , train loss 0.6926470465130277 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2096 , train loss 0.6925596925947402 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2097 , train loss 0.6952770170238283 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2098 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2099 , train loss 0.6930146929290559 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2100 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2101 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2102 , train loss 0.6925293289952807 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2103 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2104 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2105 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2106 , train loss 0.6931762877437804 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2107 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2108 , train loss 0.69248376124435 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2109 , train loss 0.6928514639536539 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2110 , train loss 0.6927251948250664 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2111 , train loss 0.6927934454547034 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2112 , train loss 0.6930324832598368 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2113 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2114 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2115 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2116 , train loss 0.692956965830591 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2117 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2118 , train loss 0.6930591149462594 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2119 , train loss 0.6930658171574274 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2120 , train loss 0.6931092109945085 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2121 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2122 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2123 , train loss 0.6932199895381927 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2124 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2125 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2126 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2127 , train loss 0.6929709282186296 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2128 , train loss 0.6926966408888499 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2129 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2130 , train loss 0.6931489606698354 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2131 , train loss 0.6922729214032491 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2132 , train loss 0.6929128633605109 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2133 , train loss 0.6930827697118124 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2134 , train loss 0.6943735480308533 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2135 , train loss 0.6947914196385278 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2136 , train loss 0.6924680521090826 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2137 , train loss 0.6939246224032508 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2138 , train loss 0.6930566844013002 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2139 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2140 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2141 , train loss 0.6931471824645996 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2142 , train loss 0.6923542453183068 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2143 , train loss 0.6936049809058508 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2144 , train loss 0.6925767146878772 , val loss 0.6931471824645996\nValidation loss decreased (0.693147 --> 0.693147).  Saving model ...\nEpoch : 2145 , train loss 0.6928552091121674 , val loss 0.6933725333462158\nEarlyStopping counter: 1 out of 20\nEpoch : 2146 , train loss 0.6931616084443198 , val loss 0.6932368325069547\nEarlyStopping counter: 2 out of 20\nEpoch : 2147 , train loss 0.6930457221137153 , val loss 0.6934108290200433\nEarlyStopping counter: 3 out of 20\nEpoch : 2148 , train loss 0.6949831760591931 , val loss 0.6933389737581214\nEarlyStopping counter: 4 out of 20\nEpoch : 2149 , train loss 0.6931471824645996 , val loss 0.6933389737581214\nEarlyStopping counter: 5 out of 20\nEpoch : 2150 , train loss 0.6927060236533483 , val loss 0.6934681528558334\nEarlyStopping counter: 6 out of 20\nEpoch : 2151 , train loss 0.6934236470195982 , val loss 0.6932488322878877\nEarlyStopping counter: 7 out of 20\nEpoch : 2152 , train loss 0.6931471824645996 , val loss 0.6932488322878877\nEarlyStopping counter: 8 out of 20\nEpoch : 2153 , train loss 0.6924168583419588 , val loss 0.6934436311324438\nEarlyStopping counter: 9 out of 20\nEpoch : 2154 , train loss 0.6928550402323405 , val loss 0.6935878340154886\nEarlyStopping counter: 10 out of 20\nEpoch : 2155 , train loss 0.6926992403136359 , val loss 0.693718503539761\nEarlyStopping counter: 11 out of 20\nEpoch : 2156 , train loss 0.6929857863320245 , val loss 0.6935295490548015\nEarlyStopping counter: 12 out of 20\nEpoch : 2157 , train loss 0.6942322022385068 , val loss 0.69328390310208\nEarlyStopping counter: 13 out of 20\nEpoch : 2158 , train loss 0.6922669841183556 , val loss 0.6935661950459083\nEarlyStopping counter: 14 out of 20\nEpoch : 2159 , train loss 0.6919624143176608 , val loss 0.6935757696628571\nEarlyStopping counter: 15 out of 20\nEpoch : 2160 , train loss 0.69251073565748 , val loss 0.6935549651583036\nEarlyStopping counter: 16 out of 20\nEpoch : 2161 , train loss 0.6924203220340941 , val loss 0.6937714256346226\nEarlyStopping counter: 17 out of 20\nEpoch : 2162 , train loss 0.6935392005576028 , val loss 0.6938071679323912\nEarlyStopping counter: 18 out of 20\nEpoch : 2163 , train loss 0.6931962420543035 , val loss 0.6937937075272202\nEarlyStopping counter: 19 out of 20\nEpoch : 2164 , train loss 0.6931756056017346 , val loss 0.6937791804472605\nEarlyStopping counter: 20 out of 20\nStopping\n","output_type":"stream"}]},{"cell_type":"code","source":"##plotting\nimport matplotlib.pyplot as plt\nfig = plt.figure(figsize=(10,10))\nplt.plot(train_loss, label='Training Loss')\nplt.plot(valid_loss,label='Validation Loss')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-01-17T16:17:17.157870Z","iopub.execute_input":"2023-01-17T16:17:17.158437Z","iopub.status.idle":"2023-01-17T16:17:17.441850Z","shell.execute_reply.started":"2023-01-17T16:17:17.158389Z","shell.execute_reply":"2023-01-17T16:17:17.440682Z"},"trusted":true},"execution_count":37,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 720x720 with 1 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAAmAAAAI/CAYAAADQs2XyAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAABQYElEQVR4nO3deZgU1d328fs3OzsIg8qioLKIsukAimjAREQxgnGDaBSN66OSmJioSVQeEqNG85KYGH00GhMXcEmiGFCMCu4Lg7IrioAyoGyyLzPTM+f9o6ubmp7ume6Z7poZ+H6ui4vp6qrqU13VVXefc+q0OecEAACA4GQ1dAEAAAD2NwQwAACAgBHAAAAAAkYAAwAACBgBDAAAIGAEMAAAgIDlNHQBUtGhQwfXrVu3hi4GAABArebNm7fROVcY77kmFcC6deum4uLihi4GAABArczsi0TP0QQJAAAQMAIYAABAwAhgAAAAAWtSfcAAANiXlZeXq6SkRHv27GnooiAFBQUF6tKli3Jzc5NehgAGAEAjUVJSolatWqlbt24ys4YuDpLgnNOmTZtUUlKi7t27J70cTZAAADQSe/bsUfv27QlfTYiZqX379inXWhLAAABoRAhfTU9d9hkBDAAASJI2bdqkAQMGaMCAATrooIPUuXPn6OOysrIaly0uLtbEiRNrfY2hQ4empaxz5szRGWeckZZ1NQT6gAEAAElS+/btNX/+fEnSpEmT1LJlS91www3R50OhkHJy4keHoqIiFRUV1foa77zzTlrK2tRRAwYAABKaMGGCrrrqKg0ZMkQ///nP9cEHH+j444/XwIEDNXToUC1btkxS1RqpSZMm6dJLL9Xw4cN12GGH6d57742ur2XLltH5hw8frnPOOUe9e/fWBRdcIOecJGnmzJnq3bu3jj32WE2cODGlmq6pU6eqb9++Ovroo3XjjTdKkioqKjRhwgQdffTR6tu3r6ZMmSJJuvfee9WnTx/169dP48aNq/+blQJqwAAAQI1KSkr0zjvvKDs7W9u2bdObb76pnJwcvfLKK/rFL36hf/7zn9WW+eSTTzR79mxt375dvXr10tVXX11tmIaPPvpIS5YsUadOnXTCCSfo7bffVlFRka688kq98cYb6t69u8aPH590OdeuXasbb7xR8+bNU7t27TRy5Eg999xz6tq1q9asWaPFixdLkrZs2SJJuvPOO7Vy5Url5+dHpwWFAAYAQCP0vy8s0dK129K6zj6dWuu27x6V8nLnnnuusrOzJUlbt27VxRdfrM8++0xmpvLy8rjLjB49Wvn5+crPz1fHjh21bt06denSpco8gwcPjk4bMGCAVq1apZYtW+qwww6LDukwfvx4Pfjgg0mVc+7cuRo+fLgKC8O/f33BBRfojTfe0C233KIVK1bouuuu0+jRozVy5EhJUr9+/XTBBRdo7NixGjt2bMrvS33QBAkAAGrUokWL6N+33HKLRowYocWLF+uFF15IOPxCfn5+9O/s7GyFQqE6zZMO7dq104IFCzR8+HA98MADuuyyyyRJM2bM0DXXXKMPP/xQgwYNytjrx0MNGAAAjVBdaqqCsHXrVnXu3FmS9Oijj6Z9/b169dKKFSu0atUqdevWTU899VTSyw4ePFgTJ07Uxo0b1a5dO02dOlXXXXedNm7cqLy8PJ199tnq1auXLrzwQlVWVmr16tUaMWKEhg0bpmnTpmnHjh1q27Zt2rcpHgIYAABI2s9//nNdfPHF+s1vfqPRo0enff3NmjXTX/7yF40aNUotWrTQoEGDEs776quvVmnWfOaZZ3TnnXdqxIgRcs5p9OjRGjNmjBYsWKBLLrlElZWVkqQ77rhDFRUVuvDCC7V161Y55zRx4sTAwpckWeSOg6agqKjIFRcXN3QxAADIiI8//lhHHnlkQxejwe3YsUMtW7aUc07XXHONevTooeuvv76hi1WjePvOzOY55+KOzUEfMAAA0Kg89NBDGjBggI466iht3bpVV155ZUMXKe1oggQAAI3K9ddf3+hrvOqLGjAAAICAEcAAAAACRgADAAAIGAEMAAAgYEkFMDMbZWbLzGy5md0U5/kpZjbf+/epmW3xph9qZh9605eY2VW+ZY41s0XeOu81M0vbVtXRfbOXa8yf32roYgAA0CBGjBihWbNmVZn2hz/8QVdffXXCZYYPH67IEFGnn3563N9UnDRpku65554aX/u5557T0qVLo49vvfVWvfLKKymUPj7/j4Q3JrUGMDPLlnSfpNMk9ZE03sz6+Odxzl3vnBvgnBsg6U+S/uU99ZWk473pQyTdZGadvOful3S5pB7ev1H13pp62rijVCs27mzoYgAA0CDGjx+vadOmVZk2bdq0pH8Qe+bMmXUezDQ2gE2ePFnf+c536rSupiCZGrDBkpY751Y458okTZM0pob5x0uaKknOuTLnXKk3PT/yemZ2sKTWzrn3XHgk2H9IGlu3TQAAAOlwzjnnaMaMGSorK5MkrVq1SmvXrtWJJ56oq6++WkVFRTrqqKN02223xV2+W7du2rhxoyTp9ttvV8+ePTVs2DAtW7YsOs9DDz2kQYMGqX///jr77LO1a9cuvfPOO5o+fbp+9rOfacCAAfr88881YcIEPfvss5LCI94PHDhQffv21aWXXqrS0tLo691222065phj1LdvX33yySdJb+vUqVPVt29fHX300brxxhslSRUVFZowYYKOPvpo9e3bV1OmTJEk3XvvverTp4/69euncePGpfiuxpdMAOssabXvcYk3rRozO1RSd0mv+aZ1NbOF3jrucs6t9ZYvSWadAAAgGAcccIAGDx6sF198UVK49uu8886Tmen2229XcXGxFi5cqNdff10LFy5MuJ558+Zp2rRpmj9/vmbOnKm5c+dGn/ve976nuXPnasGCBTryyCP18MMPa+jQoTrzzDN19913a/78+Tr88MOj8+/Zs0cTJkzQU089pUWLFikUCun++++PPt+hQwd9+OGHuvrqq2tt5oxYu3atbrzxRr322muaP3++5s6dq+eee07z58/XmjVrtHjxYi1atEiXXHKJJOnOO+/URx99pIULF+qBBx5I6T1NJN0DsY6T9KxzriIywTm3WlI/r+nxOTN7NpUVmtkVkq6QpEMOOSSdZQUAoPF68Sbp60XpXedBfaXT7qxxlkgz5JgxYzRt2jQ9/PDDkqSnn35aDz74oEKhkL766istXbpU/fr1i7uON998U2eddZaaN28uSTrzzDOjzy1evFi/+tWvtGXLFu3YsUOnnnpqjeVZtmyZunfvrp49e0qSLr74Yt1333368Y9/LCkc6CTp2GOP1b/+9a9Eq6li7ty5Gj58uAoLCyVJF1xwgd544w3dcsstWrFiha677jqNHj1aI0eOlCT169dPF1xwgcaOHauxY8cm9Rq1SaYGbI2krr7HXbxp8YyT1/wYy6v5WizpRG/5Lr6nE67TOfegc67IOVcUeaMAAEBmjBkzRq+++qo+/PBD7dq1S8cee6xWrlype+65R6+++qoWLlyo0aNHa8+ePXVa/4QJE/TnP/9ZixYt0m233Vbn9UTk5+dLkrKzsxUKheq1rnbt2mnBggUaPny4HnjgAV122WWSpBkzZuiaa67Rhx9+qEGDBtX7daTkasDmSuphZt0VDknjJH0/diYz6y2pnaR3fdO6SNrknNttZu0kDZM0xTn3lZltM7PjJL0v6SKFO+8DAACp1pqqTGnZsqVGjBihSy+9NNr5ftu2bWrRooXatGmjdevW6cUXX9Tw4cMTruOkk07ShAkTdPPNNysUCumFF16I/p7j9u3bdfDBB6u8vFxPPPGEOncO90Bq1aqVtm/fXm1dvXr10qpVq7R8+XIdccQReuyxx/Stb32rXts4ePBgTZw4URs3blS7du00depUXXfdddq4caPy8vJ09tlnq1evXrrwwgtVWVmp1atXa8SIERo2bJimTZumHTt21Plmg4haA5hzLmRm10qaJSlb0iPOuSVmNllSsXNuujfrOEnTvE71EUdK+r2ZOUkm6R7nXKQ+9X8kPSqpmaQXvX8Nz9U+CwAA+7Lx48frrLPOit4R2b9/fw0cOFC9e/dW165ddcIJJ9S4/DHHHKPzzz9f/fv3V8eOHTVo0KDoc7/+9a81ZMgQFRYWasiQIdHQNW7cOF1++eW69957o53vJamgoEB/+9vfdO655yoUCmnQoEG66qqrqr1mTV599VV16bK34e2ZZ57RnXfeqREjRsg5p9GjR2vMmDFasGCBLrnkElVWVkqS7rjjDlVUVOjCCy/U1q1b5ZzTxIkT6x2+JMmq5qXGraioyEXGGsmEyS8s1TPFq7Xof2tujwYAIBM+/vhjHXnkkQ1dDNRBvH1nZvOcc0Xx5mckfAAAgIARwAAAAAJGAAMAAAgYAQwAgEakKfXNRlhd9hkBDACARqKgoECbNm0ihDUhzjlt2rRJBQUFKS2X7pHwAQBAHXXp0kUlJSXasGFDQxcFKSgoKKgyzEUyCGAx+M4BAGgoubm56t69e0MXAwGgCdLHrKFLAAAA9gcEMAAAgIARwAAAAAJGAAMAAAgYAQwAACBgBDAAAICAEcAAAAACRgCLwejDAAAg0whgPgwDBgAAgkAAAwAACBgBDAAAIGAEMAAAgIARwAAAAAJGAAMAAAgYAQwAACBgBLAYjAIGAAAyjQDmYwwEBgAAAkAAAwAACBgBDAAAIGAEMAAAgIARwAAAAAJGAAMAAAgYAQwAACBgBLAYjoHAAABAhhHAfIyBwAAAQAAIYAAAAAEjgAEAAASMAAYAABAwAhgAAEDACGAAAAABI4ABAAAEjAAWw4mBwAAAQGYRwHwYBQwAAASBAAYAABAwAhgAAEDACGAAAAABI4ABAAAEjAAGAAAQMAJYDMcoFAAAIMMIYH6MQwEAAAJAAAMAAAgYAQwAACBgBDAAAICAEcAAAAACRgADAAAIGAEMAAAgYASwGAwDBgAAMi2pAGZmo8xsmZktN7Ob4jw/xczme/8+NbMt3vQBZvaumS0xs4Vmdr5vmUfNbKVvuQHp2qi6MgYCAwAAAcipbQYzy5Z0n6RTJJVImmtm051zSyPzOOeu981/naSB3sNdki5yzn1mZp0kzTOzWc65Ld7zP3POPZueTQEAAGgakqkBGyxpuXNuhXOuTNI0SWNqmH+8pKmS5Jz71Dn3mff3WknrJRXWr8gAAABNWzIBrLOk1b7HJd60aszsUEndJb0W57nBkvIkfe6bfLvXNDnFzPKTLjUAAEATlu5O+OMkPeucq/BPNLODJT0m6RLnXKU3+WZJvSUNknSApBvjrdDMrjCzYjMr3rBhQ5qLCwAAELxkAtgaSV19j7t40+IZJ6/5McLMWkuaIemXzrn3ItOdc1+5sFJJf1O4qbMa59yDzrki51xRYSGtlwAAoOlLJoDNldTDzLqbWZ7CIWt67Exm1ltSO0nv+qblSfq3pH/Edrb3asVkZiZprKTFddwGAACAJqXWuyCdcyEzu1bSLEnZkh5xzi0xs8mSip1zkTA2TtI055x/KK3zJJ0kqb2ZTfCmTXDOzZf0hJkVSjJJ8yVdlYbtqT8GAgMAABlWawCTJOfcTEkzY6bdGvN4UpzlHpf0eIJ1npx0KQNiDAMGAAACwEj4AAAAASOAAQAABIwABgAAEDACGAAAQMAIYAAAAAEjgAEAAASMABbDMRAYAADIMAKYD8OAAQCAIBDAAAAAAkYAAwAACBgBDAAAIGAEMAAAgIARwAAAAAJGAAMAAAgYASyGYxgwAACQYQQwH2MgMAAAEAACGAAAQMAIYAAAAAEjgAEAAASMAAYAABAwAhgAAEDACGAxGIUCAABkGgHMx8Q4FAAAIPMIYAAAAAEjgAEAAASMAAYAABAwAhgAAEDACGAAAAABI4ABAAAEjAAWwzlGAgMAAJlFAPMxhgEDAAABIIABAAAEjAAGAAAQMAIYAABAwAhgAAAAASOAAQAABIwABgAAEDACWAxGAQMAAJlGAPNhGDAAABAEAhgAAEDACGAAAAABI4ABAAAEjAAGAAAQMAIYAABAwAhgAAAAASOAxXAMBAYAADKMAOZnjAQGAAAyjwAGAAAQMAIYAABAwAhgAAAAASOAAQAABIwABgAAEDACGAAAQMAIYAAAAAEjgPkwChgAAAhCUgHMzEaZ2TIzW25mN8V5foqZzff+fWpmW7zpA8zsXTNbYmYLzex83zLdzex9b51PmVle2rYKAACgEas1gJlZtqT7JJ0mqY+k8WbWxz+Pc+5659wA59wASX+S9C/vqV2SLnLOHSVplKQ/mFlb77m7JE1xzh0habOkH9Z/cwAAABq/ZGrABkta7pxb4ZwrkzRN0pga5h8vaaokOec+dc595v29VtJ6SYVmZpJOlvSst8zfJY2t0xYAAAA0MckEsM6SVvsel3jTqjGzQyV1l/RanOcGS8qT9Lmk9pK2OOdCta0TAABgX5PuTvjjJD3rnKvwTzSzgyU9JukS51xlKis0syvMrNjMijds2JDGogIAADSMZALYGkldfY+7eNPiGSev+THCzFpLmiHpl86597zJmyS1NbOc2tbpnHvQOVfknCsqLCxMorgAAACNWzIBbK6kHt5di3kKh6zpsTOZWW9J7SS965uWJ+nfkv7hnIv095JzzkmaLekcb9LFkp6v60akW7h4AAAAmVFrAPP6aV0raZakjyU97ZxbYmaTzexM36zjJE1zVdPLeZJOkjTBN0zFAO+5GyX9xMyWK9wn7OH6b079GAOBAQCAAOTUPovknJspaWbMtFtjHk+Ks9zjkh5PsM4VCt9hCQAAsF9hJHwAAICAEcAAAAACRgADAAAIGAEMAAAgYASwOBiFAgAAZBIBzMfEOBQAACDzCGAAAAABI4ABAAAEjAAGAAAQMAIYAABAwAhgAAAAASOAAQAABIwAFgfDgAEAgEwigPkYw4ABAIAAEMAAAAACRgADAAAIGAEMAAAgYAQwAACAgBHAAAAAAkYAAwAACBgBLA7nGAkMAABkDgHMh2HAAABAEAhgAAAAASOAAQAABIwABgAAEDACGAAAQMAIYAAAAAEjgAEAAASMABYHo4ABAIBMIoD5GAOBAQCAABDAAAAAAkYAAwAACBgBDAAAIGAEMAAAgIARwAAAAAJGAAMAAAgYASwOx0BgAAAggwhgPsZAYAAAIAAEMAAAgIARwAAAAAJGAAMAAAgYAQwAACBgBDAAAICAEcDicGIcCgAAkDkEMAAAgIARwAAAAAJGAAMAAAgYAQwAACBgBDAAAICAEcAAAAACRgADAAAIGAEsDscwYAAAIIMIYD5mDV0CAACwPyCAAQAABCypAGZmo8xsmZktN7Ob4jw/xczme/8+NbMtvudeMrMtZvafmGUeNbOVvuUG1HdjAAAAmoKc2mYws2xJ90k6RVKJpLlmNt05tzQyj3Puet/810ka6FvF3ZKaS7oyzup/5px7to5lBwAAaJKSqQEbLGm5c26Fc65M0jRJY2qYf7ykqZEHzrlXJW2vVykBAAD2IckEsM6SVvsel3jTqjGzQyV1l/Rakq9/u5kt9Jow85NcBgAAoElLdyf8cZKedc5VJDHvzZJ6Sxok6QBJN8abycyuMLNiMyvesGFD+koKAADQQJIJYGskdfU97uJNi2ecfM2PNXHOfeXCSiX9TeGmznjzPeicK3LOFRUWFiazagAAgEYtmQA2V1IPM+tuZnkKh6zpsTOZWW9J7SS9m8wLm9nB3v8maaykxUmWOWNMDAQGAAAyr9a7IJ1zITO7VtIsSdmSHnHOLTGzyZKKnXORMDZO0jTnqo4jb2ZvKtzU2NLMSiT90Dk3S9ITZlYoySTNl3RVujYKAACgMas1gEmSc26mpJkx026NeTwpwbInJph+cnJFBAAA2LcwEj4AAEDACGAAAAABI4ABAAAEjAAGAAAQMAJYHFXv4wQAAEgvApiPMQwYAAAIAAEMAAAgYAQwAACAgBHAAAAAAkYAAwAACBgBDAAAIGAEMAAAgIARwOJwYiAwAACQOQQwH4YBAwAAQSCAAQAABIwABgAAEDACGAAAQMAIYAAAAAEjgAEAAASMAAYAABAwAlgcjmHAAABABhHAfIyBwAAAQAAIYAAAAAEjgAEAAASMAAYAABAwAhgAAEDACGAAAAABI4DFwSgUAAAgkwhgPibGoQAAAJlHAAMAAAgYAQwAACBgBDAAAICAEcAAAAACRgADAAAIGAEMAAAgYASwOJxjJDAAAJA5BDAfYxgwAAAQAAIYAABAwAhgAAAAASOAAQAABIwABgAAEDACGAAAQMAIYAAAAAEjgMXBKGAAACCTCGAAAAABI4ABAAAEjAAGAAAQMAIYAABAwAhgAAAAASOAAQAABIwABgAAEDACWByOgcAAAEAGEcB8zKyhiwAAAPYDBDAAAICAJRXAzGyUmS0zs+VmdlOc56eY2Xzv36dmtsX33EtmtsXM/hOzTHcze99b51NmllfvrQEAAGgCag1gZpYt6T5Jp0nqI2m8mfXxz+Ocu945N8A5N0DSnyT9y/f03ZJ+EGfVd0ma4pw7QtJmST+s0xYAAAA0McnUgA2WtNw5t8I5VyZpmqQxNcw/XtLUyAPn3KuStvtnsHBnq5MlPetN+ruksckXGwAAoOlKJoB1lrTa97jEm1aNmR0qqbuk12pZZ3tJW5xzodrWCQAAsK9Jdyf8cZKedc5VpGuFZnaFmRWbWfGGDRvStVoAAIAGk0wAWyOpq+9xF29aPOPka36swSZJbc0sp7Z1OucedM4VOeeKCgsLk1h1GjAOGAAAyKBkAthcST28uxbzFA5Z02NnMrPektpJere2FTrnnKTZks7xJl0s6flkC50pjAIGAACCUGsA8/ppXStplqSPJT3tnFtiZpPN7EzfrOMkTfPCVZSZvSnpGUnfNrMSMzvVe+pGST8xs+UK9wl7uP6bAwAA0Pjl1D6L5JybKWlmzLRbYx5PSrDsiQmmr1D4DksAAID9CiPhAwAABIwABgAAEDACGAAAQMAIYHE4xqEAAAAZRADzMcahAAAAASCAAQAABIwABgAAEDACGAAAQMAIYAAAAAEjgAEAAASMAAYAABAwAlgcjmHAAABABhHAfBgGDAAABIEABgAAEDACGAAAQMAIYAAAAAEjgAEAAASMAAYAABAwAhgAAEDACGBxMAwYAADIJAKYjxkjgQEAgMwjgAEAAASMAAYAABAwAhgAAEDACGAAAAABI4ABAAAEjAAGAAAQMAJYHM4xEhgAAMgcApgPw4ABAIAgEMAAAAACRgADAAAIGAEMAAAgYAQwAACAgBHAAAAAAkYAAwAACBgBLA5GAQMAAJlEAPNhGDAAABAEAhgAAEDACGAAAAABI4ABAAAEjAAGAAAQMAIYAABAwAhgAAAAASOAxeEYCAwAAGQQAczPGAkMAABkHgEMAAAgYAQwAACAgBHAAAAAAkYAAwAACBgBDAAAIGAEsDicGIcCAABkDgHMh0EoAABAEAhgAAAAASOAAQAABCypAGZmo8xsmZktN7Ob4jw/xczme/8+NbMtvucuNrPPvH8X+6bP8dYZWa5jWrYIAACgkcupbQYzy5Z0n6RTJJVImmtm051zSyPzOOeu981/naSB3t8HSLpNUpEkJ2met+xmb/YLnHPF6doYAACApiCZGrDBkpY751Y458okTZM0pob5x0ua6v19qqT/Oue+8ULXfyWNqk+BAQAAmrpkAlhnSat9j0u8adWY2aGSukt6Lcll/+Y1P95ixi9hAwCA/UO6O+GPk/Ssc64iiXkvcM71lXSi9+8H8WYysyvMrNjMijds2JDGotaAYcAAAEAGJRPA1kjq6nvcxZsWzzjtbX6scVnnXOT/7ZKeVLipsxrn3IPOuSLnXFFhYWESxa076uAAAEAQkglgcyX1MLPuZpancMiaHjuTmfWW1E7Su77JsySNNLN2ZtZO0khJs8wsx8w6eMvlSjpD0uL6bQoAAEDTUOtdkM65kJldq3CYypb0iHNuiZlNllTsnIuEsXGSpjnnnG/Zb8zs1wqHOEma7E1roXAQy/XW+Yqkh9K3WQAAAI1XrQFMkpxzMyXNjJl2a8zjSQmWfUTSIzHTdko6NpWCAgAA7CsYCR8AACBgBDAAAICAEcAAAAACRgCLg2HAAABAJhHAfEwMBAYAADKPAAYAABAwAhgAAEDACGAAAAABI4ABAAAEjAAGAAAQMAIYAABAwAhgcTgGAgMAABlEAPMxhgEDAAABIIABAAAEjAAGAAAQMAIYAABAwAhgAAAAASOAAQAABIwABgAAEDACWBxODAQGAAAyhwDmwzBgAAAgCAQwAACAgBHAAAAAAkYAAwAACBgBDAAAIGAEMAAAgIARwOJwjEIBAAAyiADmY4xDAQAAAkAAAwAACBgBDAAAIGAEMAAAgIARwAAAAAJGAAMAAAgYAQwAACBgBLA4GAYMAABkEgHMx8RAYAAAIPMIYAAAAAEjgAEAAASMAAYAABAwAhgAAEDACGAAAAABI4ABAAAEjAAWh3OMBAYAADKHAObHMGAAACAABDAAAICAEcAAAAACRgADAAAIGAEMAAAgYAQwAACAgBHAAAAAAkYAi4NhwAAAQCYRwHwYBgwAAASBAAYAABAwAhgAAEDAkgpgZjbKzJaZ2XIzuynO81PMbL7371Mz2+J77mIz+8z7d7Fv+rFmtshb571mRgsgAADYL+TUNoOZZUu6T9IpkkokzTWz6c65pZF5nHPX++a/TtJA7+8DJN0mqUiSkzTPW3azpPslXS7pfUkzJY2S9GKatgsAAKDRSqYGbLCk5c65Fc65MknTJI2pYf7xkqZ6f58q6b/OuW+80PVfSaPM7GBJrZ1z7znnnKR/SBpb140AAABoSpIJYJ0lrfY9LvGmVWNmh0rqLum1Wpbt7P1d6zoBAAD2NenuhD9O0rPOuYp0rdDMrjCzYjMr3rBhQ7pWCwAA0GCSCWBrJHX1Pe7iTYtnnPY2P9a07Brv71rX6Zx70DlX5JwrKiwsTKK4dcd9AAAAIAjJBLC5knqYWXczy1M4ZE2PncnMektqJ+ld3+RZkkaaWTszaydppKRZzrmvJG0zs+O8ux8vkvR8PbcFAACgSaj1LkjnXMjMrlU4TGVLesQ5t8TMJksqds5Fwtg4SdO8TvWRZb8xs18rHOIkabJz7hvv7/+R9KikZgrf/cgdkAAAYL9QawCTJOfcTIWHivBPuzXm8aQEyz4i6ZE404slHZ1sQQEAAPYVjIQPAAAQMAIYAABAwAhgAAAAASOAxbH3NgIAAID0I4D5MAoYAAAIAgEMAAAgYAQwAACAgBHAAAAAAkYAAwAACBgBDAAAIGAEsDicGIcCAABkDgHMxxiHAgAABIAABgAAEDACGAAAQMAIYAAAAAEjgAEAAASMAAYAABAwAhgAAEDACGBxOIYBAwAAGUQA82EcMAAAEAQCGAAAQMAIYAAAAAEjgAEAAASMAAYAABAwAhgAAEDACGAAAAABI4DFwTBgAAAgkwhgPiYGAgMAAJlHAAMAAAgYASwOx28RAQCADCKA+WzcUSpJ+r/XVzRwSQAAwL6MAOazbtseSdILC9c2cEkAAMC+jAAGAAAQMAKYD12/AABAEAhgAAAAASOA+RjDgAEAgAAQwAAAAAJGAIuDvmAAACCTCGA+RhskAAAIAAHMhxHwAQBAEAhgcVARBgAAMokABgAAEDACWBy0RAIAgEwigPnQCR8AAASBAOZDJ3wAABAEAlgcVIQBAIBMIoABAAAEjAAGAAAQMAIYAABAwAhgcdAXHwAAZBIBzCcyDIUTCQwAAGQOASwOE7dBAgCAzCGAAQAABIwA5sNArAAAIAgEMB/yFwAAjd8jb63UZ+u2N3Qx6iWpAGZmo8xsmZktN7ObEsxznpktNbMlZvakb/pdZrbY+3e+b/qjZrbSzOZ7/wbUe2vqifwFAEDj5pzT5P8s1Zl/fruhi1IvObXNYGbZku6TdIqkEklzzWy6c26pb54ekm6WdIJzbrOZdfSmj5Z0jKQBkvIlzTGzF51z27xFf+acezadG1QflV4VGD9FBABA4xRprdpdXtGwBamnZGrABkta7pxb4ZwrkzRN0piYeS6XdJ9zbrMkOefWe9P7SHrDORdyzu2UtFDSqPQUPXNoikys200z9KNpHzV0MQAA+6nKfeQinUwA6yxpte9xiTfNr6eknmb2tpm9Z2aRkLVA0igza25mHSSNkNTVt9ztZrbQzKaYWX4dtyFt9pF9mrSPv9qmnaWhlJd7fv7aDJQGAIDaVaZ4rS6vqNSAyS/r+flrMlOgOkpXJ/wcST0kDZc0XtJDZtbWOfeypJmS3pE0VdK7kiJ1hjdL6i1pkKQDJN0Yb8VmdoWZFZtZ8YYNG9JUXJSGKnTaH9/UVY/Pa+iiAACQtFQHS9+2u1xbdpVr0vQlGSpR3SQTwNaoaq1VF2+aX4mk6c65cufcSkmfKhzI5Jy73Tk3wDl3iiTznpNz7isXVirpbwo3dVbjnHvQOVfknCsqLCxMZdtStj8NQ1FZGf7/g5XfNGxBAABIQbKX6ufnr9HWXeWZLUw9JBPA5krqYWbdzSxP0jhJ02PmeU7h2i95TY09Ja0ws2wza+9N7yepn6SXvccHe/+bpLGSFtdzW+otUq0Z6YRfWel010uf6Kutu9Xtphm6e9YnDVe4NIts4/4TOQEA+4Jk+oAtX79DP5o2Xz99ZkEAJaqbWu+CdM6FzOxaSbMkZUt6xDm3xMwmSyp2zk33nhtpZksVbmL8mXNuk5kVSHrT+43FbZIudM5FOh09YWaFCteKzZd0VZq3LWWRas3Ivl1QskX3z/lc877YLEm6b/bn+tmpvRuqeGkVPX5JYADQZJVXVKqi0qkgN7uhixKYZPqAbd1dJknauKM0w6Wpu1oDmCQ552Yq3JfLP+1W399O0k+8f/559ih8J2S8dZ6camEzLTZUR1J2aRO/1TWeSNjcV+4mQdP22brtWr+9VCcc0aGhi9Ko/WXOcrUuyNWFxx3a0EWp4tG3V2rkUQepU9tmDV2UpJVs3qX8nGwVtmrw+7/qZex9b2vJ2m1adefohi5KYJLpLrSnPNzPplludqOtZ2AkfJ/YnRTZx6necTFryddau2V3WsqUKZFta6wHJvYvp0x5Qxf89f2GLkaj97uXlulXzzV4b40q1m/fo0kvLNUlf5vb0EVJybC7ZmvQ7a80dDHqbcnabbXPtI9J5pq8x6s4KcjNUmWqF/GAEMDiiL3DItU7Lq58bJ7G3Ne4R+iN1Hw1lhsPykKV+uW/F2n9tj0NXRQAKYjc0LPFa/IBMi2Z69buaADLTrkSJSgEMJ/YfeoSTK95HeGZN2xvvO3O0t5taywH5uxl6/XE+1/q1ucb123CAIDGJZnrVqnXBJmfk6WKRlLREIsA5hMJTybzHoenpxJSKhpLoqlFYzse977XjaxgAGqUagsBUF/J1IBFriXZWVWbIOd98Y3WNZKWFgKYT7UasDo00zXWpF2Nr5hloUp9sWln7YukcdvWbtmtzTtpssiES/72gQ67eUZDFwP7iaZyysO+I5l6jshxmWVV+zyfff+7OuX/vZ6xsqWCAOaT6JtcKieYplID5q9p+tVzi/Stu+fEHbAuVFGp1d/s8pZJ3+sPvfM1HX/nq+lbIaJmL9vQaJqWse9rKuc8JO+LTTujndgbo1RqwMyqV4xs25P6T/BlAgHMx/8L6xt3lO7tA5ZCFXuoiZyM/KV8urhEkrSzrPpBeddLn+jE383W+m170n6ijdwmjMZh4tSPdMeLHzd0MdDE7L2hp4ELgrQor6jUt+6eo4lTP2rooiSUVA2Y93+WWaPt2kIA8/HvoqLfvKIfT5svKbWan2Rvd125cafeXr4x+RWnWbLNiW98Gi7jN7vKGu1BjPSYvmCt/u/1FQ1dDDQxTeVLJ5JTXhH+Yjzn08b728vJVIpErldzlm1gGIqmIDZgfO111EsmrJz0u9k694F3kj4ZjbhnToOOexSvlPGn7f12W1MAe+fzjWn/za3yikrdP+fzRl0VXl+hikoNv3u2Xlz0VUMXBaiTyMUt8vNmQQlVVOo/C9fWen5+fv4aLVm7NaBSNX3lFY0zrPglc5mNzPP1tj3V5jdVNooqWwJYEpLZTV9+s0tzV23OWNJ2zumJ979IWxiJF6Yqavjg7SoLJTzod5aG9P2H3tfljxWnpWwRUz/4Une99IkeemPfrZXZtiekVZt26aZ/LWroogB10lA3Hj381kpd++RHmr5gbY3z/WjafI2+962AStX0hbwasIDzdEqSuc7654l0n4lMeyT3bun+odLGzzJTwCQRwPwS7NN455en5n6p5+evqTY9tgZs+57y6G9J1sd/l67TL/+9WL97aZl2lYVqPenUKs42lVdW75MVGZLj7PvfTdgHLFJl/clX6R2ReWdpOGzuiOmb5pyr8x2Zq7/ZpZ8/uyBa5lR9/NU2vfN5+pqO/R1FG9KuOP3/4uk3aZZufT7zI7E//t4X6nbTDG3dnd5a1f3RK0vX6fH3vsjY+kMNVGPy1dZwC8XGHdxNnS5fbNrZJN7PZE7//mtx5Dx7fOWHWph/mUZkL5DWL5VyGvZnqAhgPon2abzaohv/uUg/8vqI+cWGlP954kOdff872llav7suIh3kN+0s1aTpSzRx6kf66Mu6B7t421rbiTRR6CmrY5iRpE3J/FBqzMte9fg8db95Zvx5a3Hzvxbp6eISfbDymzotf9of39T3H0pf03Fdg2C6/fDRvbWXzjn1/NWL+uubVWsev966R9v2hPSPd9N3MT/vgXd11l+q/2rE399ZJUmNZryexibZ9+WTr7fpsn8UZ/Tnixq6b6gpXLOxu2zf7aqQjPoME1ReUamz739H37p7jk79wxtpLFVmJNMHrMJXoTBt7peSpJvsUbW28F39OuMPUttDMlG8pBHAfBIdwF9s2pX0Ok783ewqjz/6coukvRfaUEWltuza+w0j2Q9Nlu0dHDbyza8+t9LGe9lNO0vV7aYZeuzdVXGbOhPVgJWFUgsRy9dvj/596h/ejJQo6eVnLVmX0uv5RWqaGvqiEZHqe5eK2GNrV1lID7+1Mm71/bsrNu0tU0WlykKV+s2MvXdEbtpRquPuqPuwIc65uMfUB6u+iX5GqswfXa7OL7lPu/bJD5Oab1T085Ue3/vL2/q/1z+vMq0xdML/7cyPdeStL6k0VPUYayw/tRYE//l5zrL1Kf0eccnm3fVuqdm4o1Qn3Pmaln29vfaZ6ymZQ87/3fbx98IBrIX2fnH5cFNuuouVMgKYT7If1VQ+1JF5IxfaW55frAGT/xt9PtWTV6Vz0TBWl/5mLy3+Wt1umqGvtlb/cC71ftT1lueX6ELvBgH/N41EL5dqiPjL7L0n8I1eDVhQ53CLvHdJvN7iNVv1z3kl0ceZOJlH3rtMtEBWVDq9uOgrTX5hqSTp7lnL9Ov/LNWsJV/XuNyesr3787N121VR6bS5njdYTJu7Wr1veUlrUvyR+oYOyr+d+bFmLGyYGyT6/+/L+uMr8fuobG+AcYyen79GH365RXe8+EmV6YnOQ4tKtmrOsvVBFE1PzV0tSdVqwRJ1KE9mSJ2Szbui/aGaAv+1ZMLf5uqMPyXf7237nvo39b/28Xqt2bJbDwbQZzeZgcMrYrrUDM+ar462Jfr4otkN2/woEcCqSOai/N0/vaV7Xl4WffzbmR/rtD8m/pYZWedL3kVv6gerqzyfbBNUJDg4SdlZ4b9Dlan3hXrWCxTxah22+frbFMf5NpToYphqE2Rs6Fz9za6aT4gJ0kldAlFWCjVgZ/zpLf30mQXRx+sz8PuepV4A27yrvFoz9dxV3+j1etwKHqp0uvqJD/XI2yt1878W6llvvLfdtdzIscdXi3DKlDf0h1c+rXcQmj4/3Gfxi401nzhf/3RDld9RrWsN4YoNO7QtDReVB99YoWuSrG2K569vrlC3m2akPIZeZaXT1t3lmvLKp3V+7fr614cleuuzcH/HLzbtjNvlQkocZr7757c04W9zM1W8uHbFBLBE56YT73qtxvX89OkFGnbXbN0+M/lx8Rav2arZnwQTOOOpjPmy/00KvzRy6aPJ7aeXFn+t5et3pF64NEvmuPJfZy7LnqFH835X5fkdap72cqWKAOaTzAV90Zqtus9Xg/PgGyv0cYLO57vLKqI1SIl+ZLosVKldZSENnPyyXv24etPa3FXf6I4XP46ORi+3N0Rc/o9iTZpe+49X3zd7eXTdrQpyJCnuxWlTnA+s+dJP7EU48s038oFP9O5t3V2uu176JNo8EHvCPusvb9d8gUrwVGnMxXnJ2q217sO61B5G5k30A+u7ykJJd2KP5Q/gf3x1b21HWahS5z7wri5+5INa1/Hykq+1qKT6bfb+E9DUD1ZruxfwIgE+kdhahOJVmwPrX3PxIx/owr++Hz3qdpVV1Om9Pfn3r+v7D70XfbynvEIrawl/sdJR4/m7WeEva6nWMOyKE5KDbk77ydMLdOHD4Zrw2GDjF/vZnbXka82uR81XyeZdyfUN9fzptc+id2JWC2AJAvzarTX3ofvnh+EvK9Pnr42etz5bt73aF6LPN+zQn1/7TM45nfGnt3RJkkEmEyL7oS6fl9o63q/btkflFZW66vF5+k4D/4xPsudu/3H5q9wnMlWceiGA+aT79PbV1t1VQsLGOCeVsopKrdy4U5t3lcf9tnXuA+/q/15fobu9E7m/CVKS/l5Lh+g95RW6e9Yy/fDvxXry/S/VMj8cwOJ9O6qtY6//gN60o1SH/WKmnnz/y2onuTnL1lc5Uf3p1c90/5zPNXHqR3phwVrNiBnzauOOsiphYfuecp34u9eidxv+3xsrov2H/BchfyiYs2y9Rt/7lp7xNRnGE3nvUhnrJnIXpv+OvE/XbY+emE+8a7aG/DZx/6jte8q1I6Z2a/uecr2wYG2V9+7BN1Zo1cad+s/Cter5qxfjruurrbu1fnvV/XTFY/P03T+Hmxv8VfOJhiyxWm653BPTjyY7yxLWmu0oDVXp01ib0iRqS5et267PvG/ZP3l6vvrcOiulwBypSVy8Zu8Xo58+vUAj7plT7T2ZNH2JBt3+iqTwid3/7d6/ze/EDJr8lznL9eT74X4ly77erkfeWhm3LJGsu2VXucpClfp03Xb95On5cfvbfLZuu/7wyqdyzmlHnCbGVJvpY5vP6tOcVtNFPXYYiisfm6dLfDUUyQbH7XvKVbzqGw27a7aG3z0n6bJt3lUeDV6xXxT8n6+6/JLHpp1luuzv4RtUTpnyRpUvRM45XfTwB7rn5U8TfjmL5ZxLKSClErojLW6x55r62l1WoSG/fVU3Pruwxvlqq62vqHRaX8M1ZsP20mp9+OKJ/XJSXlFZ5X16/L0vNHfVNzXu70dCo2p9nSBYU+qkWFRU5IqL0zvWlN/nM/+oNe8+k7H1x9O6Wa6cq/4NuUV+Tkp3TrZtkRd3+pYUqqGzs7NU4TtJt22Rl9LydV1Gkprn52hXbdtrUtvm1dffqiC32vuXzPthZmrTPLfa9HYt8lQaqqy9PDEir1keqtTO0lDC96Km96i2bfEv16ZZrizLotOa5WXXqaYqmX3WsiCnSihonpdT5ULiX0e1996pSkhr3SxXoYrULkRtmudVGapjZ2nIC9Eu+s0p8rqVlS7anB6ZFilb62a5yvLVAEa326TsrPDxn5+brdLyCrVpnltlcOGsLJOZhU/s3nnTv92tCnKVnV013Pqfi7dft+wsq1bGSDn927BlZ1m1/ZvoGI+Ked/bNs+r0pxfWl4RXV/suvxliXd8+OcPVVRGjw1/uf3aNMvV1t3lysnOknNSq2Y5ccsR+xr+98f/uSoLVYZbGGKuX60KclXhXPSzG++8Ers9+bnZapaXHd3uSFkTLdO2RZ627iqv8tqtmuVqe2R/Nc+Lvu+x7+vO0pDKQ5XRedo2z5OTtHVXWZXzUaQsEdlZWaqorFRWlik3O0ul5RXKzg7Xn0TO2eFlTVt3Vd13setq2yKv2rGRSNsWedpdWlEtGMWek2q7XvnPF5HPYGT7Y/d/2xZ5VT7DkXXHXp/8OrTKV1moMu6xJ0knZYfHWZxc/gP9q2KYtqiVJOnJy4do6OEdansb6sXM5jnniuI9lxNv4v7q8Ha52mqpdRKur2aV4QPGWczBWya1TKJntpnJOafQrt1qkVd1d5ZXVKqlJf+tt0VOjnZW7i1HfqhcHfLi370WT35Otqy8TC0t9RDQQjnKin0P4sgLlattdqVCvg6WrnR3tfcqu6xMOXGa2lrGvEZ+KEeVzlUpc/mu3cpS4vc/cjKJ/Y2x/FB4X4bKQmppUmhX9XJJUlZpaZX9YrJoU3VzV67smO2LrDe2/BV7wvs8Oq28epmT+R20rLL4+ywvJ1tl3ok3vyJb8s8T81o5vv0e+97v9N6PiMo9u5WfnaWsJI7N3OwslVdUqqCi6ok1VBFStAutt+68UHk0X1RaSDlZWdH37oDc8J2dzSrLJe9lnaq+n82ys7W7skIKSbkm5YVi3hcv78W+ZmQdzV255Du8yiv3fv6auXJlZVVUqS2KHAeVu/coy6zKMZFdHn4uJztL2ZH943vPC3KzlR2qvVnTv335se9h+d794j/GYt8XxRyvsfPnS5I3f0FluSq9vyPnJil8rLY0Rd/7/NDec1WkHJF9LXldH/Z4r7unVPk5WdHPVW55mbIqnVrk7q2ZzckyhSqdmrvyqsdbnPNoduzxHpLys3IUqgyfB6JljcyfZcoPlat1VoUqnVNeqFwtFKoSZptXlktZ4UAY2r077vsa3q5KlVulsrzPS2h3+HrTMno85YSH1Yg5J8kp/HpO0eMz8l5mZYU/4wUVVbc9NztLeaHyateBvFCZdpVVJDy/ZWeFv2Tk52QrJ1SuUEUo/Hoe/+cqWs4yqXVWDeeacqlNdni9zSrLtWtPhVrKKausVNmhyiplySotVaiisvo+rFTC/sB7dlR9H2MVV/bU70Pn6t3Ko6pMLzr0gPgLBIQA5nf8/+h7zx9a79Uc2r65pl8zTP0nvxyd9tw1J6hk8y5dN/Wj6K313do315yfjYjOc8MzC6Kd5GON7nuwZiz6Ss9edbzOeeBdSVLxr76j/JwsXfb3Yt1wai/16lb1YPr7O6t0WxJ9xFrm52jx/54qKTz6/M3/WqTzi7rqrnP6VZmv200zalzPqsmjk5rv+WtOUP+ubTX87tlatWmXnrriOPU6rL0k6Zf/XiQn6bdn9Y2uZ+Udp0fH/frsttOUm52lx977Qrd4Yxu1zM/Riz86UV3aNdOVj83Ty0vX6f5zj9FpfQ+u9tqRdc795XdU6ZwObF2gTTtKdexvXqk27z3n9tcNvk74Unjfvu7ts1lLvtaVj80Lb/udo6PzvPv5Js1a8rVuPr23/vjKZ7rsxMOUn5Olo26bVW17nrxsiIYekfw3sJUbd2rEPXMkSf+4dLCO6VmoM//8lhbG9AH74BffVsfWBVWm7S6r0JG3vlRtnf9vbH91bFWgCx9+X53bNoveqbhq8ujo+/W3CwZpRK+OWvb19irjBP35+wN1Rr9OmvfFNzr7/vBx+eRFVbdp0vQlevSdVfrF6b3125mfVHsP/nHpYF3kNe2MH3yIpn7wpQ5uU6B3b/52wvchUq4nLhsS/Umv+TedEq7lSdLW3eXq/7/hz+jHk0dFa0GKfvOKNu4o1Qc3hN/D//fyMt372vIqy0b2t3Muuh2R4z/iqblf6sZ/LtL3jums/3fegGpl/9XoI/WbGR9rwtBuKg1VaOoHq3XjqN46sUcHHd25TXT+4lXfRD/zkjRj4jD16bT3+WS3MbZ8736+SeO9fnL+55xzuvPFT/TE+19qR2lI1518hP7kbf+iSSPVMj+n1mbsiCVrt2r0vW+pZX5OlaYx/+tF3o9Pf3Oa8nL29oqJvO8//k4P/fg7PTVg8svasqtci34xUq0Kcqss+8mvR6kgN7z//rt0nS7/R7ilJC87S5/eflqVc9JD44qiz/vL8/byjbrgr+9rcPcD9PSVxyfcpir7/M7R1Z7/2TML9My8EuVkmZZPPj3uOp77aI1+/NR8nd73IO0srdDrn27QDSN76tqTe0gKN/n2uXVWteWeuuI4nf/ge1Wm+ctwwV/f09vLN+mhi4p0Sp8DJVW/Diz+5am668VP9FiCwXljtyny3kXO236fb9ih037/enS5e2Yt059nV/2sSNLPTu2la0YcEX085r63tWD1Fv3l3GP0P0+Eb3Lp37WtFqzeot+MPVoPvP65SjbXrTJk4slHVPu8xvO9gZ2rHG8NgT5gGXDR8d3UIj87+njVnaM1oGtbndGvk1besffgvvykw6osd1Sn1pKkCUO7Rf+WpB4dW+qec/tr6uXHqcgXsjq0zFerglw9deXxGtStepJv2zy5cU4iJy5JOsi7aLcsqJ7NP548Sr8ee7Qk6dxjuyS17liPXjIo+iGOfFfyv/7tZ/XVb8/qW2UZ/8k+16t295/+nXPqekDzpC4Kh7YP3/lS2CpfB3rb2i7BRbtjq+q3KQ/pvvd9Ps4LjT8f1avKPMcf3l6TzjxK+TnZ+vmo3jqgRZ5a5OdoxW9P1+e/Pb1KOSPrSFb3Di109zn99OEtp+iknoVVnovUOj32w8HVwpcUbqJcecfeC8LwXuHls7NM+blZ0Xn8Di9sISl8IZOkXF8T2w+HdY+e5Du33XtHUfP8qsfO9af01P8MP1wThnbX+MFd9eglg2Rmmn7tCXrisiFq7nvN5jGvn8ibPx+hWT8+SSf4gl7k2EhWvu/k69/uv186SJef2F2FcfZ/LDPTH8cN0J/GD6z+nHeUWoKv7ZHjwGzvIMhtmuVWCV+SdOyh7fT7c/tHH/s/L7VpUcP7efzh7fXJr0dp4aSR1cp18+lHql+XcDkin4/medlqVZCbdPiSpKM6tdGqO0er10GtEs5zRMeWklTrxfCpK47Xzaf1joYvP/++/M6RHfXTU3p6G1N9Pa3inNuk5H+NItntjz2P+eX4Pkej+4W/KJ7Rr1N0WvO8HA07ooOmnN9fC24bqTbNwtuc7BeMmt7LFnnZOvnIjkmtxy/e5yv2vTy3qIs6tMzTL07vXWX69wfHDHjq1UL491vkLfGv85mrjtfAQ9qmVM7rI/u+FrXdjBQEasAy4PxBXZVTw8XgyINbq3VBji4YUrW2LVIz5u9b8LNTe+ncoi5qlpet4w8PX6zv/F7fuMNExGrtnajyc7Kq3THo1yxvb1mH9yrU787up+/27xRnvmzlJ9iuwXECYKT8kRsIYkWqq2Mv+qk69eiDon9P/HYPLVu3PfpexZox8cRqHZyzEnwQO7auegH+0/iB0cAhhS+W8b4BJxLvdRK9dk3OLepa5fFBrQu0UFs19Yrj1KZZrnoemPhi5794RJqszSx6IozXbOuf7j8J33JGn71laFOgA1vna9220mrraNMsVz8fFT4h3/G9vbWq/bq0laQqv+iQbADrekD1W8hzslN7LxMFtqM6tdFR/hom33t2cu+O+sHxVT+3YwZ0Tul144n0OYq3/Wams4/tEh0SpVkKAaym85AUDnOJAl3kNNSuRfg8kkwgTWTYER0SDvT5z6uHJtUfqddBrRIGOf9xbWY6f3BX/f6/8YfwSBTA+nZuo4PbFOiGkb3iPp+s6CDCNdzW5Q/l5xV11ei+B6tFzBeXxy8bEv27WW62tu4uT/rz4f8kRN6aYw9tp/OLusrMNKJX6gEsXqjzX2Mk6dD2LVT8q1MkKVrbLYX71caTm52lmRNP1PINO/Sw98sb/v2Tl52lYUd0iDtsUjwLJ41MOiCner7IBAJYjBtH9VbL/GzdkmDYCEn68JZTtG7bnrjjfx1e2CJ6p2EiL/7oxLjTxwzopOcXrNVlJx6mcYNDmvLfT3X5iYdVO/DHDT5E42K/UcQRWe7gNgVaVcNo/gU5ez/UZqbzBnVNOO/e+fb+/dpPv1WlxqVDy/y4d3zGipzg8+tRDTy8V6Hu9F3Uj+7cJtpEGE/L/Jwa98+JPTpo+fod+mrrHh0Qc9IY0LVtSrUPQbn7nP769pKv4taCxvPXi4rUrUNzTfEN8hk5ViKhuHPbZpL2XkxyojVgiffVKX0O1OPvfRn9tp4s/wmzeV7dT0m5WakdR0l/A/bekytOOkw3n9Y7pRqgmkRqE3OzszRucFe9vHSdhvWovTm6LsdgotCRjDbNcnX7WUfrpB6Ftc+cwMRv99BZAztruNd8Hrv+VI+ZiP5d2mhBnCFYajqOWuXHf61WBbk1Nnsn67qTj9Dy9Tt06lEH1T6zJzZ8xSrwaqjrEhoi59kjD26V1Lk9kbw4n/38nCxNPPkIjUxhW6W955W8nCz16dRafTq11r3eMDxtmu097+blZOnH3+kZbQKvTes4taOJUAPWCF09/HDtKA3FDWBjB3TS947pogNa5CX8keD6nJzbt8zX89ecEH384EVxb5xIWuSiWlstS1Y9LyiHFbas8vjf/zNUr3+6QScc0UEzvSEnTj3qQFVUuipNbpETQzKv36Vdsyp9AiKLHNymWVra8ScM7aZH31mlf1w6WCs37tSLi79WYcuq3/gjzXSNTZvmuTp/UPK/afadSC2e7wt6JFiFKpz+c90wHdymahNmvCbIWLeecZTGDTokbu1UTXr5auyS/YYfT11qE5OR7wWeA1rkpSV8vfKTbyk/J0sdWuZr+fodumbEESnVphakeBz+57phcZvTaxPZ1JysrGq19anKzjJ169CiXuuIZ9oVx8cddqFFXrbOObaLziuqHjhyczJ74T20fQs95zuPp8PDEwbpn/NKol1EErnsxMP09vJNVbqwpEu886yZ6Sd1qDGMnPv9X+gid6t2bdesymtmZ5le+cm3VLJ5V3QA1ma52dWGxol0l0hWTopf2DKBABZHQYIL+sijDor2u+nWvrluGNlT97xctZrb/9E+s38nDYjptBikyMEdOdhP73uQZi6q/jM0yfywaSq6HtBcFx5X9YR9WGFL3Tiqar+Ai4ceqt/O/KRaTVM8L19/kspDmRsy5bbv9tEtZ/SRmemwwpZVOoxG5Genr/brxR+duHdw3UYgUntUXllZpQ9S5HiOfPOuqUkrLyerWv+lZDTLy44G4E5tm9W+QMB+OKy7dpaGNGFot7SsL9LnSZImjzk66eWGHt5e73y+Sfk5qR2HddknfvX9gpZJzfKy43ZhMDPd4+s3F3HBkEPScuHt0q6ZumcgUCZyeGHLaDN+RPGvvlOti9uIXh2rBflIX8+zj6lbv92Imr581SRet4bINcf/3P/9oEjPzV9Tpak7cl46omPLKp+bk4/sWO0nwvz7+68XFemymBstkilX0AhgcUQuMpec0E1nH9Ml+ptaRYe2i85jZrr25B7VA5hvn94bp2NukCIfmErn9MmvR4Xb2xfNTMu60zF83BUnHa4rTjo8qXmb5+VIyd/gljIzU23nl3T2GTjy4NY68uD0f0tNRfuW4Te0RV52dNtCCQaojZys4jVDpMMvTj9S5w/qmnJH+iAU5GZXu/gl49SjD9Iz81brR9/ukZZyPHRRkUo2724UTSd1de/4gckPqpvG4Deyz4F6eek6/XrM0VV+BeTJy4bEvWGlNm/deHK9ynOQV8Pc+6C6nwM6tEyuVvPQ9i1S6quaSG4dWxpmxulyE7l++Hdx3y5t1LdL1S8LiQ6B7w3sXC2A+b+YxK5Hki49obseeXvvgMk7A/p1j5oQwBJYecfp1ZobkvmgJrrjqSFE+lY5V3O/kbqGqScvG6IDWtacir7Vs1B3z1qm79ThrpuaHOE1ew7oWr9v98n6ySk9a+2n0dTcdFpvHV7YUif37qht3o0J/b2O8bEiH4W6fguuTV5Olo48uLVWpfhzQY1Zm2a5euaqoWlbX4v8nBrvJsyUdFaAnRnn5p4g3Dt+oDbtLIsOZBqRyhAw6XTsoe30z6uH1qmF5MUfnaiva/kppVQ9eskgtW6Wq/Y1tEbU9ctXjTcFJbhe9u/aViWbd1er3Tz7mC7654cl+vaRB+qDX35bi9ds1ZPvf6lXPl4fDbVS/Nqtq4YfViWATf3gS93xvcR3qgZh37qipFGqfT1e/NGJNf4od0OInGjq+0PKfkd1Dn9jG96rY1Inr6M7t6nzt6/7vn9Mwr52Qw5rr9k3DFe39sH8oOrENNViNCbN83J0sdes1qZZrv5z3TAdVks/ikzXvkTWH9QPdJw1sHO9m+iQGZ3bhi+ondrUv1m6IDc7emNJY7j7TQqHsLpIV+35oG7tNHdV+M7UA1sXJFxnpzYFWrt1T1prp+PVgPndc05/XTase7Vavt+f11+/Py/c1NixVYFO7l2gEb06amdZRZWbq/y1dWbh13NOVcY5/P6Q5PvNZgoBLE1qO6AaQryL2Xs3f1vH3VH1dwsvSqFvy1Gd2mjx/55a652e6RAZHyeRIPtg7A+SCSKRLybJ9Nuri0x1pE9kyvkDAn09JO+8oq7q2Kog2ocpXVK9W3Zf9fhlQ/St383R17X8BvAzVw/VR19uTuuXr0MOaK6lX21LOARRs7xsDTwkuYBqZtWuR5F+i81yszX1iuP08FsrVdgyX2/fdHJ0YNlqY5M1AAJYEm77bp9aD4bIBSnR+FMNIXKx9I8rdlDM3W2TvttHP4jpMF+bIMJXYzHnhuHanMKPTe9rppw/QPe++pkObb837D4yoUi96tF3pSaRU3wqX2Revv6ktDfJoOGZmUb0Tm/XBWlvyO/dAM25jUl+TrYmfruHfvHvRTXe/NK5bbNo7WEq/jR+YMLP5d3n9tOYAZ10eMwd9OnSPDdbI/scqAlDu2lA17ZxB0puDEMK7T9X0nq45ITuCZ/7+6WDtX7bHh3UpkBzbhiuLu0az11ckT5gB9fw4Um2P+z+qluHFuqm/bemrV+XtvrrxYOqTDu594EJ5m4YPQ9sVWM/EyDWc9ecEFj3hcbs+0MOyVhTXLzBvCNaFeTG/am4dMnKslqHcarP+JPpQgCrp2/5fg4mE+Pc1EeHlvm6d/xADY2plXvtp9/Syd7vd5G/0Jgc1LpAYwd00qXDEn/pQTAitZBB9ccLUkMOD4SGlZeTpbJQJTVgyLx4dx35B051++LZFU1WVpbpD+MadvgWhN11dj/dN3u5hhyW3C8sAE1BgRfAMnVHdyoIYACAarq0a17ltzuBfcHUK47TM8Uldf75q3QigO3n0jlEBQAAjdlRndroqDMbx9AzDd8LDQAAYD9DANvPUQEGAEDwCGD7OYahAAAgeASw/ZxjIAoAAAJHANvP0QQJAEDwCGAAAAABI4Dt5xiIFQCA4BHA9lPnHttFEp3wAQBoCAzEup+64dRe+uTr7Ro3qGtDFwUAgP0OAWw/dWDrAr1w3bCGLgYAAPslmiABAAACRgADAAAIGAEMAAAgYAQwAACAgBHAAAAAAkYAAwAACBgBDAAAIGAEMAAAgIARwAAAAAKWVAAzs1FmtszMlpvZTQnmOc/MlprZEjN70jf9LjNb7P073ze9u5m9763zKTPLq//mAAAANH61BjAzy5Z0n6TTJPWRNN7M+sTM00PSzZJOcM4dJenH3vTRko6RNEDSEEk3mFlrb7G7JE1xzh0habOkH6ZhewAAABq9ZGrABkta7pxb4ZwrkzRN0piYeS6XdJ9zbrMkOefWe9P7SHrDORdyzu2UtFDSKDMzSSdLetab7++SxtZrSwAAAJqIZAJYZ0mrfY9LvGl+PSX1NLO3zew9MxvlTV+gcOBqbmYdJI2Q1FVSe0lbnHOhGtYJAACwT8pJ43p6SBouqYukN8ysr3PuZTMbJOkdSRskvSupIpUVm9kVkq6QpEMOOSRNxQUAAGg4ydSArVG41iqiizfNr0TSdOdcuXNupaRPFQ5kcs7d7pwb4Jw7RZJ5z22S1NbMcmpYp7zlH3TOFTnnigoLC5PdLgAAgEYrmQA2V1IP767FPEnjJE2Pmec5hWu/5DU19pS0wsyyzay9N72fpH6SXnbOOUmzJZ3jLX+xpOfrtykAAABNQ61NkM65kJldK2mWpGxJjzjnlpjZZEnFzrnp3nMjzWypwk2MP3PObTKzAklvhvvca5ukC339vm6UNM3MfiPpI0kPp3vjAAAAGiMLV0Y1DUVFRa64uLihiwEAAFArM5vnnCuK9xwj4QMAAASMAAYAABAwAhgAAEDACGAAAAABI4ABAAAErEndBWlmGyR9keGX6SBpY4ZfA8Fin+6b2K/7Hvbpvmd/36eHOufijiLfpAJYEMysONEto2ia2Kf7Jvbrvod9uu9hnyZGEyQAAEDACGAAAAABI4BV92BDFwBpxz7dN7Ff9z3s030P+zQB+oABAAAEjBowAACAgBHAfMxslJktM7PlZnZTQ5cHyTOzVWa2yMzmm1mxN+0AM/uvmX3m/d/Om25mdq+3nxea2TENW3pIkpk9YmbrzWyxb1rK+9DMLvbm/8zMLm6IbUFYgn06yczWeJ/V+WZ2uu+5m719uszMTvVN59zcSJhZVzObbWZLzWyJmf3Im85nNUUEMI+ZZUu6T9JpkvpIGm9mfRq2VEjRCOfcAN8tzzdJetU510PSq95jKbyPe3j/rpB0f+AlRTyPShoVMy2lfWhmB0i6TdIQSYMl3Ra5EKBBPKrq+1SSpnif1QHOuZmS5J1vx0k6ylvmL2aWzbm50QlJ+qlzro+k4yRd4+0PPqspIoDtNVjScufcCudcmaRpksY0cJlQP2Mk/d37+++Sxvqm/8OFvSeprZkd3ADlg49z7g1J38RMTnUfnirpv865b5xzmyX9V/EDAAKQYJ8mMkbSNOdcqXNupaTlCp+XOTc3Is65r5xzH3p/b5f0saTO4rOaMgLYXp0lrfY9LvGmoWlwkl42s3lmdoU37UDn3Ffe319LOtD7m33ddKS6D9m3TcO1XnPUI75aD/ZpE2Nm3SQNlPS++KymjACGfcUw59wxCld3X2NmJ/mfdOHbfbnltwljH+4z7pd0uKQBkr6S9PsGLQ3qxMxaSvqnpB8757b5n+OzmhwC2F5rJHX1Pe7iTUMT4Jxb4/2/XtK/FW62WBdpWvT+X+/Nzr5uOlLdh+zbRs45t845V+Gcq5T0kMKfVYl92mSYWa7C4esJ59y/vMl8VlNEANtrrqQeZtbdzPIU7gw6vYHLhCSYWQszaxX5W9JISYsV3n+RO2sulvS89/d0SRd5d+ccJ2mrr+ocjUuq+3CWpJFm1s5r2hrpTUMjEdPf8iyFP6tSeJ+OM7N8M+uucKftD8S5uVExM5P0sKSPnXP/z/cUn9UU5TR0ARoL51zIzK5V+ADIlvSIc25JAxcLyTlQ0r/D5wXlSHrSOfeSmc2V9LSZ/VDSF5LO8+afKel0hTv57pJ0SfBFRiwzmyppuKQOZlai8B1SdyqFfeic+8bMfq3wRVuSJjvnku0EjjRLsE+Hm9kAhZuoVkm6UpKcc0vM7GlJSxW+0+4a51yFtx7OzY3HCZJ+IGmRmc33pv1CfFZTxkj4AAAAAaMJEgAAIGAEMAAAgIARwAAAAAJGAAMAAAgYAQwAACBgBDAAAICAEcAAAAACRgADAAAI2P8HFI5Qbto2jSsAAAAASUVORK5CYII=\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":"##loading model\nml = neural_net()\nml.load_state_dict(torch.load('/kaggle/working/checkpoint.pt'))","metadata":{"execution":{"iopub.status.busy":"2023-01-17T16:17:42.797120Z","iopub.execute_input":"2023-01-17T16:17:42.797540Z","iopub.status.idle":"2023-01-17T16:17:42.808244Z","shell.execute_reply.started":"2023-01-17T16:17:42.797508Z","shell.execute_reply":"2023-01-17T16:17:42.806971Z"},"trusted":true},"execution_count":39,"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","source":"##val accuracy\ncorrect = 0\n\nmodel.eval() # prep model for evaluation\n\nfor data, target in valdataloader:\n    \n    # forward pass: compute predicted outputs by passing inputs to the model\n    output = ml(data)\n    output = output.item()\n    if output > 0.5 :\n      output = 1 \n    else :\n      output = 0\n\n    \n\n\n    correct += (output == target).float().sum()\naccuracy = 100 * correct / len(valdataloader.dataset)\nprint(\"Accuracy = {}\".format(accuracy))\n    ","metadata":{"execution":{"iopub.status.busy":"2023-01-17T16:17:43.222959Z","iopub.execute_input":"2023-01-17T16:17:43.223751Z","iopub.status.idle":"2023-01-17T16:17:43.300259Z","shell.execute_reply.started":"2023-01-17T16:17:43.223699Z","shell.execute_reply":"2023-01-17T16:17:43.299294Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"Accuracy = 67.70833587646484\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}